<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Springboot启动异常日志被吞掉]]></title>
    <url>%2F%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0%2FJava%2FSpringboot-startup-error-log-be-eat-off.html</url>
    <content type="text"><![CDATA[项目中使用了springboot2.0.4，并集成了elasticsearch、dubbo等组件。在更新了创库代码后，发现突然程序启动不了，总是启动后tomcat自动关闭，日志信息如下： 很纳闷 :-(，任何异常日志都没有，出什么情况了？第一感觉是程序有问题，更新了代码导致。可是本次更新的代码量有比较大，从源代码比对分析不靠谱，需要大量时间。另外源码有问题启动日志也应该要有报错信息。是日志级别不对吗？检查配置logging.level.root已经是INFO级别了，且没有其它级别控制。尝试将logging.level.root=debug，重启发现依然没有任何异常信息展现。日志输出组件冲突了吗？由于笔者之前有过log4j框架切换logback的经验，因此怀疑pom中引入的组件传递依赖了一些不需要的jar包，导致日志组件冲突。检查pom文件发现：elasticsearch-rest-client-sniffer与dubbo组件传递依赖引入了common-logging组件。导致spring-jcl组件的org.apache.commons.logging.LogFactory没有被加载，而使用了common-logging中的org.apache.commons.logging.LogFactory类； 这是因为在JVM环境中存在同名的的class，类加载器根据加载顺序选择的。在spring-jcl的LogFactory中，实现逻辑是如果log4j2的jar包存在，这优先使用log4j2来记录日志、然后才是slf4j。而在common-logging的LogFactory中，实现逻辑是如果log4j的jar包存在，优先采用log4j、然后采用jdk的util包中的logger记录。在我们的启动环境中，我们采用通过spring-boot-starter-logging组件来输出日志，其默认采用的是slf4j-api来通过logback记录日志，log4j2组件是不存在的，于是springboot在启动时，因为类加载器的原因使用了common-logging的LogFactory，又因为dubbo、zookeeper、zkclient引入了log4j组件，导致springboot启动时的部分日志输出到了log4j，因此异常被吃掉了。解决办法在依赖的“elasticsearch-rest-client-sniffer”、“dubbo”组件中排除掉对common-logging的依赖。12345678910111213141516171819202122&lt;dependency&gt; &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch-rest-client-sniffer&lt;/artifactId&gt; &lt;version&gt;$&#123;elasticsearch.version&#125;&lt;/version&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;commons-logging&lt;/groupId&gt; &lt;artifactId&gt;commons-logging&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;dubbo&lt;/artifactId&gt; &lt;version&gt;$&#123;dubbo.version&#125;&lt;/version&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;commons-logging&lt;/groupId&gt; &lt;artifactId&gt;commons-logging&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt; 解决后重启程序，终于看到了异常原因：]]></content>
      <categories>
        <category>技术笔记</category>
        <category>Java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>springboot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HornetQ引发的一次生产环境故障]]></title>
    <url>%2F%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0%2FJava%2FA-falut-caused-by-HornetQ.html</url>
    <content type="text"><![CDATA[前段时间生产环境遇到一次故障，最终分析原因是HornetQ队列空间满造成。HornetQ是JBoss可能旗下的一款MQ产品，现已捐献给了Apache的ActiveMQ。下面将本次故障分析分享给大家，因涉及产品的信息安全问题，本文隐去了某些敏感信息。一、事故现场2018-06-01 运维接收到数据库锁等待的预警以及前台工作站JVM断连的告警，与此同时大量系统用户反馈数据保存失败。运维通过关停服务，kill数据库阻塞会话后重启启动服务，系统得到恢复没有反馈异常。案发现场留存线索有限，事故分析受阻。 2018-07-04 15:30左右，类似“6.1大案”的问题再现。现场项目经理组织运维、开发、迅速到到位，着手恢复业务与问题修复。 二、分析过程从事故现场的分析，得到线索如下：在测试0.36.8(含PDF上传功能版本)时，发现在页面单击保存按钮时，数据库能看到三条锁等待记录；回退x-server版本到0.36.1(PDF上传功能的前一个版本)问题解决；回退到0.36.8(PDF上传功能版本)问题依然存在；故障期间对其中一台x-server的负载做了一个JVM线程堆的dump文件“threaddump-1530705117569.tdump”，同时做了两个JVM CPU的抽样快照。（注：从后续分析看，这条线索非常有价值——VIP线索）分析线索1：第一反应是代码存在bug，在某些诱因下存在并发异常，导致数据库出现死锁。（因为问题是以数据库锁来暴漏出来的，因此把目光盯在了死锁上面，最后从结果来看此处方向走偏了，浪费了一些分析时间）。可是故障期间，为了避免医生在故障之前已打开某功能书写页面且一直没有关闭，可能存在编辑器自动保存的并发问题，运维兄弟关闭了生产站点，重新搭了一台独立站点供排查问题。基于独立的站点保存此数据，线索1的现象会100%的复现。在此之前与运维确认已重启过HornetQ，初步排除了HornetQ故障；运维也重启过数据库，也初步排除数据库故障的可能。是什么样的诱因会一直存在呢？如果是代码存在100%复现的bug，那也不可能从6与1号稳定运行到7月4号。无法得出结论，于是进行其它线索分析。分析线索2：不用思考就能得出一条结论，0.36.8(含PDF上传功能版本)存在bug。可是反过来思考，为什么能稳定运行一个多月不出问题呢？而且生产测试环境也一直没有问题。一定还是存在某种诱因导致的。从0.36.8(PDF上传功能版本)的代码变更内容来看，在数据保存时，新增了JMS消息的发送，数据库mapper层与mybatis的sqlmap文件变化都不大，看不出造成死锁的可能。结合自身之前项目中的一些经验，我还是把第一怀疑放在了JMS消息的发送上面，怀疑网络抖动或HornetQ假死。（从最后结果看，此处是转折点。猜对了第一“嫌疑人”，方向又调整了回来）于是协调运维兄弟在现场测试环境重现问题，尝试关闭测试环境的HornetQ，发现虽然后台保存出现jms异常，但保存正常。尝试通过网络防火墙屏蔽HornetQ端口，数据也依然保存正常。问题复现失败。此时想起之前故障期间有做过JVM线程的dump与CPU的采样，那里面会不会有jms相关的线索呢？分析线索3:通过VisualVM打开线程dump文件，搜索“jms”关键字，发现有个dubbo服务线程在WAITING状态，里面包含了jms相关信息，如下图： 而且从线程的堆栈信息来看，确实是由数据保存时触发。于是马上打开之前的两次CPU采样，查找线程&quot;DubboServerHandler-192.168.125.30:20880-thread-44&quot;的相关信息。 展开其耗时堆栈，堆栈很深，截图关键部位： 发现Semaphore.acquire()的耗时达到了17430ms，已超过了前台数据保存时，dubbo接口调用设置的10秒超时时间，这能解释现场保存数据报错时提示的dubbo超时异常信息。新的疑问又来了：HornetQ客户端在发送消息时，到底在等什么呢？为什么JMS没有结束而数据库里面却有三条锁对象呢？难道是JMS消息发送在数据库事务中？问题一步一步往下分析，分析疑问1：虽然对JMS规范是了然于心，但之前没有使用HornetQ的经验，于是请求Google，搜索“hornetq accquireCredits”: 看到第一条结果的标题立马怦然心动，看来有人也“hang”住过啊。赶紧点开阅读（在此附上文章链接，方便后续读者阅读。https://developer.jboss.org/thread/217626）： 从这篇讨论信息中，得到几个重点信息，HornetQ应该有针对队列有大小控制，超过大小后通过配置可能会阻塞消息生产者。于是马上查阅HornetQ官方文档：(http://docs.jboss.org/hornetq/2.4.0.Final/docs/user-manual/html_single/index.html) 通过官方文档，明确知道了对应的配置项，于是马上快速验证生产环境的配置： 线上配置队列容量为10485760 byte，即10MB。且开启了BLOCK(阻塞)策略。于是马上进一步验证PDF消息队列当前大小： 果然队列容量已超过10MB的限制。为什么会挤压这么多消息没处理呢？与现场同事确认，原来是之前现场部署PDF打印服务因机器中病毒，机器已关闭。到此，我们可以得出一个结论，现场因为机器中毒，关闭了我们的一台服务，导致消息不能消费，全部积压在了HornetQ上，从而导致了x-server发送消息时受到阻塞。此时有同事提问：为什么HornetQ重启了也没用呢？通过查阅源码，发现HornetQ客户端工具类中，默认都是采用“PERSISTENT”方式发送消息，即持久化方式发送。HornetQ服务端接收到持久化消息后，都会保存到磁盘来避免意外丢失。重启HornetQ服务时，消息从磁盘重新加载到内存，因此队列空间依然是满的。根据上面的结论，在本次测试环境中，通过程序将HornetQ中pdf任务队列塞满，果然此时保存数据失败，并得到与事故发生时同样的调用超时异常。观测数据库锁对象，信息与故障期间完全一致，同样是三条锁数据。至此，问题我们也已经复现。如何基于这种情况恢复业务呢？删除HornetQ的保存数据的文件夹data，并重启HornetQ。然后在前端页面保存数据，提示保存成功。分析疑问2：为什么JMS没有结束而数据库里面却有三条锁对象呢？难道是JMS消息发送在数据库事务中？翻阅源码，发现x-server项目中，事务是通过aop配置，拦截了所有实现“AutoAopService”接口的类，默认在方法开始前打开jdbc事务，并在方法正常结束提交事务、异常结束回滚事务。X数据文档保存的实现类“XServicesImpl”正是派生自“AutoAopService”，因此默认“XServicesImpl”下面的所有方法都是在完整的jdbc事务中执行，因此当JMS消息发送未完成时，数据库事务一直没有被关闭。三、事故总结 从导火索看，现场关闭了PDF服务的机器，导致了消息没有消费者，从而导致了HornetQ的积压，进一步影响到x-server的保存，并通过jdbc事务触发了数据库锁等待的报警。整个链路是雪崩式的垮塌，通过这个事件，后续的改善措施如下：现场迅速恢复PDF服务的机器。（不需要改任何程序，现场业务就能恢复）HornetQ的队列达到容量上限后，策略的配置方式请运维进行优化，从官方文档描述来看，建议设置为PAGE或FAIL，不建议设置为BLOCK或DROP。对HornetQ队列容量纳入监控，超过阈值后进行告警。x-server在保存时，将JMS消息的发送提到jdbc事务外面，并采用异步方式发送。（即使后续HornetQ发生意外，也不影响数据库和用户正常的保存流程）]]></content>
      <categories>
        <category>技术笔记</category>
        <category>Java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>MQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring MVC中@RequestBody、@ResponseBody如何接收Abstract或Interface类型的参数？]]></title>
    <url>%2F%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0%2FJava%2FAbstract-Class-With-Responsebody.html</url>
    <content type="text"><![CDATA[在使用Spring-MVC对外发布Rest接口时，在某些场景中，入参可能会希望是一个接口类型或者抽象类型。SpingMVC的序列化默认采用的是Jackson来实现入参与出参的序列化，在调用方传递一个json字符串时，如何将json字符串转换为具体的实现class呢？如果不做任何处理？你可能会得到一个类似如下的异常：12345678910Caused by: com.fasterxml.jackson.databind.JsonMappingException: Can not construct instance of com.xxx.Task, problem: abstract types either need to be mapped to concrete types, have custom deserializer, or be instantiated with additional type information at [Source: java.io.PushbackInputStream@4e40388; line: 1, column: 2] (through reference chain: java.util.ArrayList[0]) at com.fasterxml.jackson.databind.JsonMappingException.from(JsonMappingException.java:148) at com.fasterxml.jackson.databind.DeserializationContext.instantiationException(DeserializationContext.java:892) at com.fasterxml.jackson.databind.deser.AbstractDeserializer.deserialize(AbstractDeserializer.java:139) at com.fasterxml.jackson.databind.deser.std.CollectionDeserializer.deserialize(CollectionDeserializer.java:245) at com.fasterxml.jackson.databind.deser.std.CollectionDeserializer.deserialize(CollectionDeserializer.java:217) at com.fasterxml.jackson.databind.deser.std.CollectionDeserializer.deserialize(CollectionDeserializer.java:25) at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:3736) at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:2810) 那么应该如何处理呢？接下来我们以具体的示例来演示。 假设我们需要实现一个向后台添加打印任务的接口，任务的类型分为课程打印、成绩单打印。（在此我们不讨论接口设计的合理性，只讨论存技术的实现）。 首先我们定义一个任务接口： 12345678public interface PrintTask extends Serializable&#123; Type getType(); enum Type&#123; COURSE, SCORE &#125;&#125; 接着我们定义两个具体的任务实现类： 1234567891011121314151617@Data // 此注解来自`lombok`，具体作用请参考：http://projectlombok.orgpublic class CoursePrintTask implements PrintTask&#123; private String course; public Type getType()&#123; return PrintTask.Type.COURSE; &#125;&#125;@Data // 此注解来自`lombok`，具体作用请参考：http://projectlombok.orgpublic class ScorePrintTask implements PrintTask&#123; private Double score; public Type getType()&#123; return PrintTask.Type.SCORE; &#125;&#125; 定义Rest接口： 12345678910111213@RestController@RequestMapping("/task")@Slf4j // 此注解来自`lombok`，具体作用请参考：http://projectlombok.orgpublic class TaskController&#123; @RequestMapping(value = "/print", method = RequestMethod.POST) public Task print(@RequestBody Task task)&#123; log.debug("received task: &#123;&#125;", task); // TODO print ... return task; // 直接返回任务对象 &#125;&#125; 构造请求： 1curl -XPOST http://localhost:8080/task/print -d '&#123;"course": "english", "type": "SCORE"&#125;' 不出意外，如上请求将会触发后端异常，且异常堆栈与文章开头所列类似。这是因为Jackson不知道该采用哪个子类来对{&quot;course&quot;: &quot;english&quot;, &quot;type&quot;: &quot;SCORE&quot;}进行反序列化。 如何告诉Jackson采用何种类型进行序列化呢？要做的事情也特别简单，代码如下： 12345678910111213@JsonTypeInfo(use = JsonTypeInfo.Id.NAME, include = JsonTypeInfo.As.PROPERTY, property = "type")@JsonSubTypes(&#123; @JsonSubTypes.Type(value = CoursePrintTask.class, name = "COURSE"), @JsonSubTypes.Type(value = ScorePrintTask.class, name = "SCORE")&#125;)public interface PrintTask extends Serializable&#123; Type getType(); enum Type&#123; COURSE, SCORE &#125;&#125; 更多关于jackson的异常与解决方案，请参考：http://www.baeldung.com/jackson-exception]]></content>
      <categories>
        <category>技术笔记</category>
        <category>Java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>spring-mvc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis有序集合（SortedSet）的POP实现方法]]></title>
    <url>%2F%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0%2FJava%2FRedis-ZSET-POP.html</url>
    <content type="text"><![CDATA[最近在项目中遇到一个场景需要使用分布式的优先级队列，第一反应就是通过redis的sortedset数据结构来实现。但是阅读其API发现其没有类似List的LPOP与RPOP指令，但是可以根据其提供的ZRANG、ZREVRANGE、ZREM组合命令来实现POP的操作。POP的动作可以拆解成两步：获取队首元素删除队首元素为保证执行的原子性，我们可以通过定义LUA脚本来执行组合指令：12345678local result = redis.call('ZRANGE', KEYS[1], 0, 0)local element = result[1]if element then redis.call('ZREM', KEYS[1], element) return elementelse return nilend" 将上面脚本中ZRANG替换为ZREVRANGE就可以实现从对尾POP了 相关Java代码的实现，使用了spring-data-redis的RedisTemplate： 123456789101112131415161718private static final String ZSET_LPOP_SCRIPT = "local result = redis.call('ZRANGE', KEYS[1], 0, 0)\n" + "local element = result[1]\n" + "if element then\n" + " redis.call('ZREM', KEYS[1], element)\n" + " return element\n" + "else\n" + " return nil\n" + "end"; /** * 有序集合按得分的升序提取元素，提取完成后并该删除 * @param key zset的key名字 * @param resultClass 返回值类型 * @return 提取的元素，集合为空时返回null */public &lt;T&gt; T zsetLPop(String key, Class&lt;T&gt; resultClass)&#123; return redisTemplate.execute(new DefaultRedisScript&lt;&gt;(ZSET_LPOP_SCRIPT, resultClass), Collections.singletonList(key));&#125; 测试代码: 12345678910111213141516171819202122232425262728@Testpublic void testZsetLpop() &#123; String zsetKey = "test_sorted"; Set&lt;ZSetOperations.TypedTuple&lt;String&gt;&gt; initValues = new HashSet&lt;&gt;(); initValues.add(new DefaultTypedTuple&lt;&gt;("zhangsan", 1d)); initValues.add(new DefaultTypedTuple&lt;&gt;("lisi", 2d)); initValues.add(new DefaultTypedTuple&lt;&gt;("zhaoliu", 4d)); initValues.add(new DefaultTypedTuple&lt;&gt;("wangwu", 3d)); Long count = stringRedisTemplate.boundZSetOps(zsetKey).add(initValues); System.out.println("成功添加" + count + "条初始化数据"); String element; do &#123; element = redisUtil.zsetLPop(zsetKey, String.class); if (Objects.nonNull(element)) &#123; System.out.println("Get element: " + element); &#125; &#125; while (element != null);&#125;// 控制台输出// 成功添加4条初始化数据// Get element: zhangsan// Get element: lisi// Get element: wangwu// Get element: zhaoliu]]></content>
      <categories>
        <category>技术笔记</category>
        <category>Java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GraphQL入门介绍（一）]]></title>
    <url>%2F%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0%2FJava%2FGraphQL-Introduction-1.html</url>
    <content type="text"><![CDATA[什么是GraphQLGraphQL是由FaceBook提出的一种基于API的查询语言（尽管它也支持修改数据）。它能够根据描述按需获取字段数据，不会有任何冗余信息。也能够通过一个请求一次获取多个资源。GraphQL最早的实现是由FaceBook基于javascript实现，随后几乎被所有主流编程语言支持，已知服务端的实现包含了：C#/.NET、Clojure、Elixir、Erlang、GO、Groovy、Java、JavaScript、PHP、Python、Scala、Ruby，已知客户端的实现包含了：C#/.NET、GO、Java/Android、JavaScript、Swift/Objective-C iOS、Python。更多信息请参考：http://graphql.cn/code/GraphQL的优势与劣势优点：可以通过一个请求获取多个或多种资源，避免过多碎片化的请求；Restful强调的是通过资源定位URL，每种类型的资源通过POST、DELETE、PUT、GET来实现增删改查，这样对于一个稍微复杂一点的页面来说，就容易造成碎片化的请求过多。字段按需获取，节省冗余数据的加载与传输。缺点：对排序、分页等请求的支持语义上支持的还是比较别扭。一个新的语法，看起来类json却又不是json，有一定学习 成本。Quick Start本样例采用GraphQL的java实现来演示。定义POMpom.xml12345678910111213141516171819202122232425262728293031323334353637&lt;groupId&gt;com.aqlu&lt;/groupId&gt;&lt;artifactId&gt;graphql-demo&lt;/artifactId&gt;&lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt;&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.5.9.RELEASE&lt;/version&gt;&lt;/parent&gt;&lt;properties&gt; &lt;disable.checks&gt;true&lt;/disable.checks&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt;&lt;/properties&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.graphql-java&lt;/groupId&gt; &lt;artifactId&gt;graphql-java&lt;/artifactId&gt; &lt;version&gt;7.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.h2database&lt;/groupId&gt; &lt;artifactId&gt;h2&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt;&lt;/dependencies&gt; graphql-java是GrahpQL的Java实现， 使用spring-boot快速构建项目，lombok用来精简bean的编写。 定义GraphQL的Schema下面以商品SKU为例来定义我们本次演示的schema，在项目的resources目录下新建一个sku.graphqls文件： resources/sku.graphqls123456789101112131415161718192021222324252627282930type Sku &#123; id: Int name: String specs: [Spec] stocks: [Stock] price: Float&#125;type Spec &#123; id: Int skuId: Int name: String value: String&#125;type Stock &#123; id: Int skuId: Int area: String stocks: Int&#125;schema &#123; query: Query&#125;type Query &#123; allSkus: [Sku] sku(id: Int): Sku&#125; 如上所示，我们定义了一个Sku类型，其具有id、名称、规格属性、库存信息、价格字段，其中规格属性与库存信息两个字段的类型又分别引用了下面定义的Spec与Stock两个类型。 接着定义了一个schema，并指明其query使用Query类型来进行组织，Query类型中定义了两个查询方式，allSkus用来查询所有的Sku信息，返回值是Sku的列表；sku用来获取某一个Sku信息，入参需要一个id。 编写Java服务端在设计的场景中，后端的Sku结构与上面定义的Schema结构存在差异。模拟后端采用微服务架构，商品基础信息在一个微服务中、库存在另一个微服务中。 com/aqlu/graphql/demo/sku/domain/Sku.java1234567891011121314151617181920/** * SKU实体 */@Data@Entitypublic class Sku &#123; @Id private int id; private String name; @OneToMany(targetEntity = Spec.class, mappedBy = "sku", cascade = CascadeType.ALL) private List&lt;Spec&gt; specs = new ArrayList&lt;&gt;(); // 规格属性，一对多 private BigDecimal price; public void addSpec(String name, String value) &#123; specs.add(new Spec(this, name, value)); &#125;&#125; com/aqlu/graphql/demo/sku/domain/Spec.java12345678910111213141516171819202122232425/** * 规格属性 */@Data@Entity@NoArgsConstructorpublic class Spec &#123; @Id @GeneratedValue private int id; private String name; private String value; @ManyToOne @JoinColumn(name = "sku_id", nullable = false) private Sku sku; public Spec(Sku sku, String name, String value) &#123; this.sku = sku; this.name = name; this.value = value; &#125;&#125; com/aqlu/graphql/demo/sku/domain/Stock.java1234567891011121314151617181920/** * 库存实体 */@Data@NoArgsConstructor@Entitypublic class Stock &#123; @Id @GeneratedValue private Integer id; private Integer skuId; private String area; private Integer stocks; public Stock(Integer skuId, String area, Integer stocks) &#123; this.skuId = skuId; this.area = area; this.stocks = stocks; &#125;&#125; com/aqlu/graphql/demo/sku/repository/SkuRepository.java12345/** * SKU信息仓库, JPA实现 */public interface SkuRepository extends JpaRepository&lt;Sku, Integer&gt; &#123;&#125; com/aqlu/graphql/demo/sku/repository/StockRepository.java123456789/** * 库存信息仓库, JPA实现 */public interface StockRepository extends JpaRepository&lt;Sku, Integer&gt; &#123; /** * 根据sku id获取对应的库存信息 */ List&lt;Stock&gt; queryStocksBySkuId(Integer skuId);&#125; com/aqlu/graphql/demo/sku/GraphQLService.java123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354/** * GrahpQL服务 */@Servicepublic class GraphQLService &#123; @Value("classpath:sku.graphqls") private Resource schemaResource; @Autowired private SkuDataFetcher skuFetcher; @Autowired private StocksDataFetcher stocksFetcher; @Autowired private AllSkusDataFetcher allSkusFetcher; private GraphQL graphQL; /** * GrahpQL dsl 执行入口 * @param dsl GraphQL dsl查询语言 */ public ExecutionResult query(String dsl) &#123; return graphQL.execute(dsl); &#125; @PostConstruct private void loadSchema() throws IOException &#123; // 类型定义注册，加载“sku.graphqls”文件定义的schema TypeDefinitionRegistry registry = new SchemaParser().parse(schemaResource.getFile()); // 运行时接线 RuntimeWiring runtimeWiring = buildRuntimeWiring(); // 生成Schema GraphQLSchema graphQLSchema = new SchemaGenerator().makeExecutableSchema(registry, runtimeWiring); // 生成graphQL对象 this.graphQL = GraphQL.newGraphQL(graphQLSchema).build(); &#125; private RuntimeWiring buildRuntimeWiring() &#123; return RuntimeWiring.newRuntimeWiring() .type("Query", runtimeWiring -&gt; runtimeWiring.dataFetcher("allSkus", allSkusFetcher) .dataFetcher("sku", skuFetcher) ) // 分别指定Query类型中allSkus字段与sku字段对应的Fetcher .type("Sku", runtimeWiring -&gt; runtimeWiring.dataFetcher("stocks", stocksFetcher) ) // 指定schemaSku类型的stocks字段对应的Fetcher .build(); &#125;&#125; com/aqlu/graphqls/demo/sku/fetcher/AllSkusDataFetcher.java12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455/** * `allSkus`的数据提取器 */@Componentpublic class AllSkusDataFetcher implements DataFetcher&lt;List&lt;Sku&gt;&gt;&#123; @Autowired private SkuRepository skuRepository; @Override public List&lt;Sku&gt; get(DataFetchingEnvironment environment) &#123; return skuRepository.findAll(); &#125; @PostConstruct public void init()&#123; // 添加一些初始化数据 Sku sku_1 = new Sku(); sku_1.setId(1); sku_1.setName("iPhone X 银色 64G"); sku_1.setPrice(new BigDecimal("8388.00")); sku_1.addSpec("容量", "64G"); sku_1.addSpec("颜色", "银色"); Sku sku_2 = new Sku(); sku_2.setId(2); sku_2.setName("iPhone X 银色 256G"); sku_2.setPrice(new BigDecimal("9688.00")); sku_2.addSpec("容量", "256G"); sku_2.addSpec("颜色", "银色"); Sku sku_3 = new Sku(); sku_3.setId(3); sku_3.setName("iPhone X 深空灰色 64G"); sku_3.setPrice(new BigDecimal("8388.00")); sku_3.addSpec("容量", "64G"); sku_3.addSpec("颜色", "深空灰色"); Sku sku_4 = new Sku(); sku_4.setId(4); sku_4.setName("iPhone X 深空灰色 256G"); sku_4.setPrice(new BigDecimal("9688.00")); sku_4.addSpec("容量", "256G"); sku_4.addSpec("颜色", "深空灰色"); Sku sku_5 = new Sku(); sku_5.setId(5); sku_5.setName("iPhone 8 深空灰色 256G"); sku_5.setPrice(new BigDecimal("6888.00")); sku_5.addSpec("容量", "256G"); sku_5.addSpec("颜色", "深空灰色"); skuRepository.save(Arrays.asList(sku_1, sku_2, sku_3, sku_4, sku_5)); &#125;&#125; com/aqlu/graphqls/demo/sku/fetcher/SkuDataFetcher.java123456789101112131415161718192021/** * `sku`的数据提取器 */@Component@Slf4jpublic class SkuDataFetcher implements DataFetcher&lt;Sku&gt;&#123; @Autowired private SkuRepository skuRepository; @Override public Sku get(DataFetchingEnvironment env) &#123; Integer id = env.getArgument("id"); // 获取参数 try &#123; return skuRepository.findOne(id); &#125; catch (Exception e) &#123; log.error("load sku failed.errMsg:&#123;&#125;", e.getMessage()); return null; &#125; &#125;&#125; com/aqlu/graphqls/demo/sku/fetcher/StocksDataFetcher.java123456789101112131415161718192021222324252627282930/** * `stocks`的数据提取器 */@Componentpublic class StocksDataFetcher implements DataFetcher&lt;List&lt;Stock&gt;&gt; &#123; @Autowired private StockRepository stockRepository; @Override public List&lt;Stock&gt; get(DataFetchingEnvironment env) &#123; Sku sku = env.getSource(); // 从上下文环境中获取源对象 return stockRepository.queryStocksBySkuId(sku.getId()); &#125; @PostConstruct private void init() &#123; // 添加初始化数据 Stock stock_1 = new Stock(1, "华东", 10); Stock stock_2 = new Stock(1, "华南", 20); Stock stock_3 = new Stock(1, "华北", 30); Stock stock_4 = new Stock(2, "华东", 40); Stock stock_5 = new Stock(2, "华南", 50); Stock stock_6 = new Stock(3, "华东", 60); Stock stock_7 = new Stock(3, "华北", 70); Stock stock_8 = new Stock(4, "华东", 80); stockRepository.save(Arrays.asList(stock_1, stock_2, stock_3, stock_4, stock_5, stock_6, stock_7, stock_8)); &#125;&#125; com/aqlu/graphql/demo/sku/SkuController.java123456789101112131415161718192021/** * sku控制器 */@RestController@RequestMapping("/sku")public class SkuController &#123; @Autowired private GraphQLService graphQLService; @PostMapping("/query") public ResponseEntity query(@RequestBody String dsl) &#123; ExecutionResult result = graphQLService.query(dsl); if (result.getErrors().isEmpty()) &#123; return ResponseEntity.ok(result); &#125; else &#123; return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR).body(result); &#125; &#125;&#125; com/aqlu/graphql/demo/GraphqlDemoApplication.java1234567@SpringBootApplicationpublic class GraphqlDemoApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(GraphqlDemoApplication.class, args); &#125;&#125; resources/application.properties12345# 开启h2控制台spring.h2.console.enabled=true# 打印sqlspring.jpa.show-sql=true 执行查询启动GraphqlDemoApplication。 1. 查询所有sku的id与name信息 123456$ curl -XPOST -H 'Content-Type:application/json;charset=UTF-8' 'http://localhost:8080/sku/query' -d '&#123; allSkus &#123; id name &#125;&#125;' 返回： 12345678910111213141516171819202122232425262728&#123; "data": &#123; "allSkus": [ &#123; "id": 1, "name": "iPhone X 银色 64G" &#125;, &#123; "id": 2, "name": "iPhone X 银色 256G" &#125;, &#123; "id": 3, "name": "iPhone X 深空灰色 64G" &#125;, &#123; "id": 4, "name": "iPhone X 深空灰色 256G" &#125;, &#123; "id": 5, "name": "iPhone 8 深空灰色 256G" &#125; ] &#125;, "errors": [], "extensions": null&#125; 2. 查询所有sku的id、name、price信息 1234567$ curl -XPOST -H 'Content-Type:application/json;charset=UTF-8' 'http://localhost:8080/sku/query' -d '&#123; allSkus &#123; id name price &#125;&#125;' 返回： 123456789101112131415161718192021222324252627282930313233&#123; "data": &#123; "allSkus": [ &#123; "id": 1, "name": "iPhone X 银色 64G", "price": 8388 &#125;, &#123; "id": 2, "name": "iPhone X 银色 256G", "price": 9688 &#125;, &#123; "id": 3, "name": "iPhone X 深空灰色 64G", "price": 8388 &#125;, &#123; "id": 4, "name": "iPhone X 深空灰色 256G", "price": 9688 &#125;, &#123; "id": 5, "name": "iPhone 8 深空灰色 256G", "price": 6888 &#125; ] &#125;, "errors": [], "extensions": null&#125; 执行上面两个查询时，注意观察后台日志，你会发现每次请求执行的sql为： Hibernate: select sku0_.id as id1_0_, sku0_.name as name2_0_, sku0_.price as price3_0_ from sku sku0_，说明只查询了sku表。 3. 查询所有sku的id、name、price、specs信息 1234567891011$ curl -XPOST -H 'Content-Type:application/json;charset=UTF-8' 'http://localhost:8080/sku/query' -d '&#123; allSkus &#123; id name price specs &#123; name value &#125; &#125;&#125;' 查询语句添加了specs字段，同时为spec指定了name与value字段。 注意：因为specs返回的是Spec类型列表，因此必须要进一步指定需要提取的Spec的字段名。否则会返回语法验证错误：Validation error of type SubSelectionRequired: Sub selection required for type null of field specs 返回： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283&#123; "data": &#123; "allSkus": [ &#123; "id": 1, "name": "iPhone X 银色 64G", "price": 8388, "specs": [ &#123; "name": "容量", "value": "64G" &#125;, &#123; "name": "颜色", "value": "银色" &#125; ] &#125;, &#123; "id": 2, "name": "iPhone X 银色 256G", "price": 9688, "specs": [ &#123; "name": "容量", "value": "256G" &#125;, &#123; "name": "颜色", "value": "银色" &#125; ] &#125;, &#123; "id": 3, "name": "iPhone X 深空灰色 64G", "price": 8388, "specs": [ &#123; "name": "容量", "value": "64G" &#125;, &#123; "name": "颜色", "value": "深空灰色" &#125; ] &#125;, &#123; "id": 4, "name": "iPhone X 深空灰色 256G", "price": 9688, "specs": [ &#123; "name": "容量", "value": "256G" &#125;, &#123; "name": "颜色", "value": "深空灰色" &#125; ] &#125;, &#123; "id": 5, "name": "iPhone 8 深空灰色 256G", "price": 6888, "specs": [ &#123; "name": "容量", "value": "256G" &#125;, &#123; "name": "颜色", "value": "深空灰色" &#125; ] &#125; ] &#125;, "errors": [], "extensions": null&#125; 执行查询时，注意观察后台日志，你会发现此次请求执行的sql为： 1234567&gt; Hibernate: select sku0_.id as id1_0_, sku0_.name as name2_0_, sku0_.price as price3_0_ from sku sku0_&gt; Hibernate: select specs0_.sku_id as sku_id4_1_0_, specs0_.id as id1_1_0_, specs0_.id as id1_1_1_, specs0_.name as name2_1_1_, specs0_.sku_id as sku_id4_1_1_, specs0_.value as value3_1_1_ from spec specs0_ where specs0_.sku_id=?&gt; Hibernate: select specs0_.sku_id as sku_id4_1_0_, specs0_.id as id1_1_0_, specs0_.id as id1_1_1_, specs0_.name as name2_1_1_, specs0_.sku_id as sku_id4_1_1_, specs0_.value as value3_1_1_ from spec specs0_ where specs0_.sku_id=?&gt; Hibernate: select specs0_.sku_id as sku_id4_1_0_, specs0_.id as id1_1_0_, specs0_.id as id1_1_1_, specs0_.name as name2_1_1_, specs0_.sku_id as sku_id4_1_1_, specs0_.value as value3_1_1_ from spec specs0_ where specs0_.sku_id=?&gt; Hibernate: select specs0_.sku_id as sku_id4_1_0_, specs0_.id as id1_1_0_, specs0_.id as id1_1_1_, specs0_.name as name2_1_1_, specs0_.sku_id as sku_id4_1_1_, specs0_.value as value3_1_1_ from spec specs0_ where specs0_.sku_id=?&gt; Hibernate: select specs0_.sku_id as sku_id4_1_0_, specs0_.id as id1_1_0_, specs0_.id as id1_1_1_, specs0_.name as name2_1_1_, specs0_.sku_id as sku_id4_1_1_, specs0_.value as value3_1_1_ from spec specs0_ where specs0_.sku_id=?&gt; 说明同时查询了sku、spec两张表。 4. 查询指定id的sku信息，包含id、name、price、specs、stocks字段 123456789101112131415$ curl -XPOST -H 'Content-Type:application/json;charset=UTF-8' 'http://localhost:8080/sku/query' -d '&#123; sku(id: 1) &#123; id name price specs &#123; name value &#125; stocks &#123; area stocks &#125; &#125;&#125;' 注意查询语句sku指令后面跟了参数id:1 返回： 1234567891011121314151617181920212223242526272829303132333435&#123; "data": &#123; "sku": &#123; "id": 1, "name": "iPhone X 银色 64G", "price": 8388, "specs": [ &#123; "name": "容量", "value": "64G" &#125;, &#123; "name": "颜色", "value": "银色" &#125; ], "stocks": [ &#123; "area": "华东", "stocks": 10 &#125;, &#123; "area": "华南", "stocks": 20 &#125;, &#123; "area": "华北", "stocks": 30 &#125; ] &#125; &#125;, "errors": [], "extensions": null&#125; 执行查询时，注意观察后台日志，你会发现此次请求执行的sql为： 1234&gt; Hibernate: select sku0_.id as id1_0_, sku0_.name as name2_0_, sku0_.price as price3_0_ from sku sku0_&gt; Hibernate: select specs0_.sku_id as sku_id4_1_0_, specs0_.id as id1_1_0_, specs0_.id as id1_1_1_, specs0_.name as name2_1_1_, specs0_.sku_id as sku_id4_1_1_, specs0_.value as value3_1_1_ from spec specs0_ where specs0_.sku_id=?&gt; Hibernate: select stock0_.id as id1_2_, stock0_.area as area2_2_, stock0_.sku_id as sku_id3_2_, stock0_.stocks as stocks4_2_ from stock stock0_ where stock0_.sku_id=?&gt; 说明同时查询了sku、spec、stock三张表。 小结通过上面的Demo，可以发现如果是一个相对简单的需求场景，使用GraphQL反而会增加编写DataFetcher的工作量，因此简单的场景使用Rest风格更合适。 但Graphql带来的灵活度非常高，统一的一个http查询接口，根据不同的dsl即可以得到想要的数据，甚至能为每个字段定义单独的DataFetcher。 在大家都在追逐微服务架构的今天，GraphQL的诞生恰逢其实，它能很好的承担起中台的角色，根据不同类型前台页面的展示逻辑，编制各个后台的微服务业务，真正做到按需加载、减少交互次数。 完整示例代码： https://github.com/aqlu/graphql-demo]]></content>
      <categories>
        <category>技术笔记</category>
        <category>Java</category>
      </categories>
      <tags>
        <tag>graphql</tag>
        <tag>微服务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ES日志收集定期清理与备份]]></title>
    <url>%2F%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0%2F%E5%B7%A5%E5%85%B7%E7%B1%BB%2FES-Clean-And-Backup.html</url>
    <content type="text"><![CDATA[按天清理索引1234$ crontab -e## 每日凌晨1点定时删除30天之前的`logstash-YYYY.MM.DD`索引0 1 * * * /home/kibana/indexClean.sh es.zyouwei.com logstash -30 indexClean.sh123456789101112#!/bin/sh# before $1 dayindex="$2-"`date -d "$3 day " +%Y.%m.%d`beginDate=`date "+%Y-%m-%d %H:%M:%S"`echo "$beginDate: begin to deleting index: $index" &gt;&gt; /tmp/indexClean.logresult=`curl --connect-timeout 10 -m 20 -XDELETE $1/$index`endDate=`date "+%Y-%m-%d %H:%M:%S"`echo "$endDate: exec result is : $result" &gt;&gt; /tmp/indexClean.log 按天关闭索引1234$ crontab -e## 每日凌晨1点05分定时关闭7天之前的`logstash-YYYY.MM.DD`索引5 1 * * * /home/kibana/indexClose.sh es.zyouwei.com logstash -7 indexClose.sh123456789101112#!/bin/sh# before 1 weekindex="$2-"`date -d "$3 day " +%Y.%m.%d`beginDate=`date "+%Y-%m-%d %H:%M:%S"`echo "$beginDate: begin to closing index: $index" &gt;&gt; /tmp/indexClose.logresult=`curl --connect-timeout 10 -m 20 -XPOST $1/$index/_close`endDate=`date "+%Y-%m-%d %H:%M:%S"`echo "$endDate: exec result is : $result" &gt;&gt; /tmp/indexClose.log 按月清理索引1234$ crontab -e## 每月1号凌晨2点定时删除3月之前的`logstash-srv-YYYY.MM`索引0 2 1 * * /home/kibana/indexCleanByMonth.sh es.zyouwei.com logstash-srv -3 indexCleanByMonth.sh123456789101112#!/bin/sh# before $3 monthindex="$2-"`date -d "$3 month " +%Y.%m`beginDate=`date "+%Y-%m-%d %H:%M:%S"`echo "$beginDate: begin to deleting index: $index" &gt;&gt; /tmp/indexCleanByMonth.logresult=`curl --connect-timeout 10 -m 20 -XDELETE $1/$index`endDate=`date "+%Y-%m-%d %H:%M:%S"`echo "$endDate: exec result is : $result" &gt;&gt; /tmp/indexCleanByMonth.log 备份索引到其它ES集群12$ crontab -e0 3 * * * /home/kibana/indexBackup.sh indexBackup.sh123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131#!/usr/bin/env python# -*- coding:utf-8 -*-__author__ = 'aqlu'import urllib2import json# import tracebackimport loggingimport osfrom datetime import timedelta, datetimelogging.basicConfig(filename=os.path.join(os.getcwd(), 'indexBackup.log'), level=logging.DEBUG, filemode='a', format='%(asctime)s - %(levelname)s: %(message)s')log = logging.getLogger('root')# noinspection PyBroadExceptiondef backup_index(index, src_es_host, dest_es_host, close_index_name): snapshot_name = 'snapshot-' + index + '_' + datetime.now().strftime('%Y%m%d-%H%M%S') # step.0 validate index try: validate_url = 'http://' + src_es_host + '/' + index validate_request = urllib2.Request(validate_url) validate_request.get_method = lambda: 'HEAD' validate_response = urllib2.urlopen(validate_request) except urllib2.HTTPError, e: if e.code == 404: log.info("index [%s] is not exists", index) else: if validate_response.getcode() == 200: # step.1 backup index # print datetime.now().strftime('%Y-%m-%d %H:%M:%S'), ' begin to backup index [', indexName, ']' log.debug('begin to backup index [' + index + ']') backup_url = 'http://' + src_es_host + "/_snapshot/log_backup/" \ + snapshot_name + '?wait_for_completion=true' backup_param = &#123;"indices": index&#125; try: backup_request = urllib2.Request(backup_url, json.dumps(backup_param)) backup_request.get_method = lambda: 'PUT' # 设置HTTP的访问方式 backup_response = urllib2.urlopen(backup_request) backup_result = json.loads(backup_response.read(), 'utf-8') except: log.exception('exception') send_alarm("告警：备份索引" + index + "异常【xxx】") else: log.debug("backup index end. result: %s", backup_result) if backup_result['snapshot']['state'] == 'SUCCESS': # if snapshot success then # step.2 delete index log.debug('begin to delete index [' + index + ']') try: delete_url = 'http://' + src_es_host + "/" + index delete_request = urllib2.Request(delete_url) delete_request.get_method = lambda: 'DELETE' delete_response = urllib2.urlopen(delete_request) delete_result = json.loads(delete_response.read()) log.debug("delete index end. result: %s", delete_result) except: log.exception('exception') send_alarm("告警：删除索引" + index + "异常【xxx】") # step.3 restore index log.debug('begin to restore snapshot [' + snapshot_name + ']') try: restore_url = 'http://' + dest_es_host + '/_snapshot/log_backup/' + snapshot_name \ + '/_restore?wait_for_completion=true' restore_request = urllib2.Request(restore_url) restore_request.get_method = lambda: 'POST' restore_response = urllib2.urlopen(restore_request) restore_result = json.loads(restore_response.read()) except: log.exception('exception') send_alarm("告警：还原快照" + snapshot_name + "异常【xxx】") else: log.debug("restore snapshot end. result: %s", restore_result) if restore_result['snapshot']['shards']['failed'] == 0: # if no failed shards then # step.4 delete snapshot log.debug('begin to delete snapshot [' + snapshot_name + ']') try: del_snapshot_url = 'http://' + dest_es_host + '/_snapshot/log_backup/' + snapshot_name del_snapshot_request = urllib2.Request(del_snapshot_url) del_snapshot_request.get_method = lambda: 'DELETE' del_snapshot_response = urllib2.urlopen(del_snapshot_request) del_snapshot_result = json.loads(del_snapshot_response.read()) log.debug("delete snapshot end. result: %s", del_snapshot_result) if close_index_name # close index for before 1 month on backup cluster close_index(close_index_name, dest_es_host) except: log.exception('exception') send_alarm("告警：删除快照" + snapshot_name + "异常【xxx】")# noinspection PyBroadExceptiondef close_index(index, esHost): log.debug("begin to close index [%s]", index) try: url = 'http://' + esHost + '/' + index + '/_close' request = urllib2.Request(url) request.get_method = lambda: 'POST' response = urllib2.urlopen(request) result = json.loads(response.read()) log.debug("close index [%s] end. result: %s", index, result) except: log.exception('exception') send_alarm("告警：关闭索引" + index + "异常【xxx】")# noinspection PyBroadExceptiondef send_alarm(content): log.debug("send sms, content: [%s]", content) try: ## TODO implements alarm as sms、call、dingTalk ...... log.debug("send sms end. result: %s", result) except: log.exception('exception')before1Week = datetime.now() + timedelta(days=-7)before1month = datetime.now() + timedelta(days=-30)indexName = 'logstash-' + before1Week.strftime('%Y.%m.%d')closeIndexName = 'logstash-' + before1month.strftime('%Y.%m.%d')srcEsHost = 'es.zyouwei.com'destEsHost = 'es-bak.zyouwei.com'backup_index(indexName, srcEsHost, destEsHost, closeIndexName)]]></content>
      <categories>
        <category>技术笔记</category>
        <category>工具类</category>
      </categories>
      <tags>
        <tag>elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[高可用HBASE搭建]]></title>
    <url>%2F%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%2F2017-5-Hbase-Deployed.html</url>
    <content type="text"><![CDATA[HDFS的高可用搭建不在本篇中描述，请参考《Hadoop HDFS与YARN高可用安装》。节点规划hostnameip安装服务zk1192.168.1.1zookeeperzk2192.168.1.2zookeeperzk3192.168.1.3zookeeper提示：zookeeper集群安装不在本文描述范围， 请自行参考其他文档。（zookeeper集群也可以跟datanode安装在一起）hostnameip安装服务master1192.168.1.10HMastermaster2192.168.1.11HMasterregion1192.168.1.12HRegionServerregion2192.168.1.13HRegionServerHBase Master 用于协调多个 Region Server，侦测各个 Region Server 之间的状态，并平衡 Region Server 之间的负载。HBase Master 还有一个职责就是负责分配 Region 给 Region Server。HBase 允许多个 Master 节点共存，但是这需要 Zookeeper 的帮助。不过当多个 Master 节点共存时，只有一个 Master 是提供服务的，其他的 Master 节点处于待命的状态。当正在工作的 Master 节点宕机时，其他的 Master 则会接管 HBase 的集群。对于一个 Region Server 而言，其包括了多个 Region。Region Server 的作用只是管理表格，以及实现读写操作。Client 直接连接 Region Server，并通信获取 HBase 中的数据。对于 Region 而言，则是真实存放 HBase 数据的地方，也就说 Region 是 HBase 可用性和分布式的基本单位。如果当一个表格很大，并由多个 CF 组成时，那么表的数据将存放在多个 Region 之间，并且在每个 Region 中会关联多个存储的单元（Store）。对于 HBase 而言，Zookeeper 的作用是至关重要的。首先 Zookeeper 是作为 HBase Master 的 HA 解决方案。也就是说，是 Zookeeper 保证了至少有一个 HBase Master 处于运行状态。并且 Zookeeper 负责 Region 和 Region Server 的注册。其实 Zookeeper 发展到目前为止，已经成为了分布式大数据框架中容错性的标准框架。不光是 HBase，几乎所有的分布式大数据相关的开源框架，都依赖于 Zookeeper 实现 HA。安装前准备操作系统：CentOS 6.5 x86_64 JDK：1.8.0_74-b02 (JDK的安装本文不进行描述) Hbase: Hbase-1.2.5-bin.tar.gz Zookeeper: 3.4.6安装过程1. 主机名修改登录nna节点，修改/etc/hosts文件，在后面追加如下内容：12345678910111213192.168.1.1 zk1192.168.1.2 zk2192.168.1.3 zk3192.168.1.10 master1192.168.1.11 master2192.168.1.12 region1192.168.1.13 region2# hdfs服务器地址，hdfs搭建请参考《Hadoop HDFS与YARN高可用安装》192.168.1.4 nna192.168.1.5 nns192.168.1.6 dn1192.168.1.7 dn2192.168.1.8 dn3 分发到其它主机，下面以zk1为例： 1scp /etc/hosts root@zk1:/etc 2. 创建hbase用户登录除zk外的主机，分别创建hbase用户： 1useradd hbase 3. 添加ssh信任123456789101112su - hbasessh-keygen -t rsa #一直按回车键，直到交互结束。会在 ~/.ssh/ 目录下生成 id_rsa.pub 文件cat ~/.ssh/id_rsa.pub # 拷贝里面的内容# 在所有主机中重复以上步骤touch ~/.ssh/authorized_keys &amp;&amp; chmod 644 ~/.ssh/authorized_keysvi ~/.ssh/authorized_keys # 将所有主机的 id_rsa.pub 文件中的内容都拷贝到此文件中，并将此文件分发到所有主机。注意是hbase用户下的 ~/.ssh/authorized_keys 文件 做完上述动作后，可以使用如下命令验证免密登录是否设置成功，若登录过程不需要输入密码则已设置成功： 12su - hbasessh master1 有必要在所有机器上都相互验证下。 4. 线程数与打开文件句柄数修改Hbase会在同一时间使用很多的文件句柄.大多数linux系统使用的默认值1024是不能满足的。 编辑/etc/security/limits.conf： 1234* soft nofile 65535* hard nofile 65535* soft nproc 16384* hard nproc 16384 若操作系统为centos6.5，还需编辑/etc/security/limits.d/90-nproc.conf： 1234* soft nproc 1024root soft nproc unlimitedhbase soft nproc 16384hbase hard nproc 16384 除zk外的主机，请都完成上述设置。 5. 关闭防火墙由于hbase的节点之间需要通信（RPC机制），这样一来就需要监听对应的端口，这里我就直接将防火墙关闭了，命令如下： 1chkconfig iptables off 6. 开启时钟同步各个节点的时间如果不同步，会出现启动异常，或其他原因。 1service ntpd start 7. 环境变量配置12345export JAVA_HOME=/usr/lib/javaexport HBASE_HOME=/home/hbase/hbaseexport HADOOP_HOME=/home/hbase/hadoop #配置hadoop变量，否则hbase不识别hdfs集群名export HBASE_MANAGES_ZK=false #不使用hbase自带的zookeeper，使用搭建的zk集群。export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin 核心文件配置 将hbase-1.2.5-bin.tar.gz上传到/home/hbase下，并解压。拷贝hdfs节点上的hadoop安装目录到/home/hbase。 12su - hbase &amp;&amp; cd /home/hbasetar -xzvf hbase-1.2.5-bin.tar.gz &amp;&amp; ln -s hbase-1.2.5 hbase $HBASE_HOME/conf/hbase-site.xml1234567891011121314&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://cluster1/hbase&lt;/value&gt;&lt;!-- hdfs集群名需与hadoop中的配置保持一致 --&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;zk1:2181,zk2:2181,zk3:2181&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; $HBASE_HOME/conf/regionservers12region1region2 将配置好的hbase与hadoop复制到其它节点； 1234567scp -r hbase master2:~/scp -r hbase region1:~/scp -r hbase region2:~/scp -r hadoop master2:~/scp -r hadoop region1:~/scp -r hadoop region2:~/ 启动 登录到master1： 12su - hbase$HBASE_HOME/bin/start-hbase.sh # 此脚本会在master1上启动HMaster，region1与region2上启动HRegionServer 登录到master2: 12su - hbase$HBASE_HOME/bin/hbase-daemon.sh start master # 此脚本会在master2上启动HMaster 访问：http://192.168.1.10:16010]]></content>
      <categories>
        <category>技术笔记</category>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[高可用HDFS搭建]]></title>
    <url>%2F%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%2FHDFS-Deployed.html</url>
    <content type="text"><![CDATA[HDFS+HA架构图上图大致架构包括：利用共享存储来在两个NN间同步edits信息。以前的HDFS是share nothing but NN，现在NN又share storage，这样其实是转移了单点故障的位置，但中高端的存储设备内部都有各种RAID以及冗余硬件，包括电源以及网卡等，比服务器的可靠性还是略有提高。通过NN内部每次元数据变动后的flush操作，加上NFS的close-to-open，数据的一致性得到了保证。DN同时向两个NN汇报块信息。这是让Standby NN保持集群的最新状态的必须步骤。用于监视和控制NN进程的FailoverController进程。显然，我们不能在NN进程内部进行心跳等信息同步，最简单的原因，一次FullGC就可以让NN挂起十几分钟，所以，必须要有一个独立的短小精悍的watchdog来专门负责监控。这也是一个松耦合的设计，便于扩展或更改，目前版本里是用ZooKeeper（简称ZK）来做同步锁，但用户可以方便的把这个Zookeeper FailoverController（简称ZKFC）替换为其他的HA方案或leader选举方案。隔离（Fencing），防止脑裂，就是保证在任何时候只有一个主NN，包括三个方面：共享存储fencing，确保只有一个NN可以写入edits。客户端fencing，确保只有一个NN可以响应客户端的请求。DN fencing，确保只有一个NN向DN下发命令，譬如删除块，复制块等等。节点规划hostnameip安装服务zk1192.168.1.1zookeeperzk2192.168.1.2zookeeperzk3192.168.1.3zookeeper提示：zookeeper集群安装不在本文描述范围， 请自行参考其他文档。（zookeeper集群也可以跟datanode安装在一起）hostnameip安装服务nna192.168.1.4NameNode、DFSZKFailoverControllernns192.168.1.5NameNode、DFSZKFailoverControllerdn1192.168.1.6JournalNode、NodeManager、DataNodedn2192.168.1.7JournalNode、NodeManager、DataNodedn3192.168.1.8JournalNode、NodeManager、DataNodenamenode服务器: 运行namenode的服务器应该有相同的硬件配置。在HA集群中,standby状态的namenode可以完成checkpoint操作，因此没必要配置Secondary namenode、CheckpointNode、BackupNode。如果真的配置了还会报错。journalnode服务器: 运行的journalnode进程非常轻量，可以部署在其他的服务器上。注意：必须允许至少3个节点。当然可以运行更多，但是必须是奇数个,如3,5,7,9个等等。当运行N个节点时，系统可以容忍至少(N-1)/2个节点失败而不影响正常运行。安装前准备操作系统：CentOS 6.5 x86_64 JDK：1.8.0_74-b02 (JDK的安装本文不进行描述) Hadoop：hadoop-2.8.0.tar.gz Zookeeper: 3.4.6安装过程1. 主机名修改登录nna节点，修改/etc/hosts文件，在后面追加如下内容：12345678192.168.1.1 zk1192.168.1.2 zk2192.168.1.3 zk3192.168.1.4 nna192.168.1.5 nns192.168.1.6 dn1192.168.1.7 dn2192.168.1.8 dn3 分发到其它主机，下面以zk1为例： 1scp /etc/hosts root@zk1:/etc 2. 创建hadoop用户登录除zk外的主机，分别创建hadoop用户： 1useradd hadoop 3. 添加ssh信任1234567891011su - hadoopssh-keygen -t rsa #一直按回车键，直到交互结束。会在 ~/.ssh/ 目录下生成 id_rsa.pub 文件cat ~/.ssh/id_rsa.pub # 拷贝里面的内容# 在所有主机中重复以上步骤touch ~/.ssh/authorized_keys &amp;&amp; chmod 644 ~/.ssh/authorized_keysvi ~/.ssh/authorized_keys # 将所有主机的 id_rsa.pub 文件中的内容都拷贝到此文件中，并将此文件分发到所有主机。注意是hadoop用户下的 ~/.ssh/authorized_keys 文件 做完上述动作后，可以使用如下命令验证免密登录是否设置成功，若登录过程不需要输入密码则已设置成功： 12su - hadoopssh nns 有必要在所有机器上都相互验证下。 4. 线程数与打开文件句柄数修改Hdaoop会在同一时间使用很多的文件句柄.大多数linux系统使用的默认值1024是不能满足的。 编辑/etc/security/limits.conf： 1234* soft nofile 65535* hard nofile 65535* soft nproc 16384* hard nproc 16384 若操作系统为centos6.5，还需编辑/etc/security/limits.d/90-nproc.conf： 1234* soft nproc 1024root soft nproc unlimitedhadoop soft nproc 16384hadoop hard nproc 16384 除zk外的主机，请都完成上述设置。 5. 关闭防火墙由于hadoop的节点之间需要通信（RPC机制），这样一来就需要监听对应的端口，这里我就直接将防火墙关闭了，命令如下： 1chkconfig iptables off 6. 开启时钟同步各个节点的时间如果不同步，会出现启动异常，或其他原因。 1service ntpd start 7. 环境变量配置123export JAVA_HOME=/usr/lib/javaexport HADOOP_HOME=/home/hadoop/hadoopexport PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin 8. 核心文件配置创建文件夹： 123456mkdir -p /home/hadoop/tmpmkdir -p /home/hadoop/data/tmp/journalmkdir -p /home/hadoop/data/dfs/namemkdir -p /home/hadoop/data/dfs/datamkdir -p /home/hadoop/data/yarn/localmkdir -p /home/hadoop/log/yarn 解压安装包： 12tar -xzvf hadoop-2.8.0.tar.gzln -s hadoop-2.8.0 hadoop $HADOOP_HOME/etc/hadoop/core-site.xml 1234567891011121314151617181920212223242526&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://cluster1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;io.file.buffer.size&lt;/name&gt; &lt;value&gt;131072&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/tmp&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.hadoop.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.hadoop.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt; &lt;value&gt;zk1:2181,zk2:2181,zk3:2181&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; $HADOOP_HOME/etc/hadoop/hdfs-site.xml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.nameservices&lt;/name&gt; &lt;value&gt;cluster1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.ha.namenodes.cluster1&lt;/name&gt; &lt;value&gt;nna,nns&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.cluster1.nna&lt;/name&gt; &lt;value&gt;nna:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.cluster1.nns&lt;/name&gt; &lt;value&gt;nns:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.cluster1.nna&lt;/name&gt; &lt;value&gt;nna:50070&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.cluster1.nns&lt;/name&gt; &lt;value&gt;nns:50070&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt; &lt;value&gt;qjournal://dn1:8485;dn2:8485;dn3:8485/cluster1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.client.failover.proxy.provider.cluster1&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt; &lt;value&gt;sshfence&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt; &lt;value&gt;/home/hadoop/.ssh/id_rsa&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/data/tmp/journal&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/data/dfs/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/data/dfs/data&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.journalnode.http-address&lt;/name&gt; &lt;value&gt;0.0.0.0:8480&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.journalnode.rpc-address&lt;/name&gt; &lt;value&gt;0.0.0.0:8485&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt; &lt;value&gt;zk1:2181,zk2:2181,zk3:2181&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; $HADOOP_HOME/etc/hadoop/mapred-site.xml 1234567891011121314&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;nna:10020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;nna:19888&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 1234567891011121314&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://cluster1/hbase&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;172.17.18.9:2181,172.17.18.112:2181,172.17.17.19:2181&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; $HADOOP_HOME/etc/hadoop/slaves 123dn1dn2dn3 将配置好的hadoop复制到其它节点： 1234scp -r $HADOOP_HOME/ nns:~/scp -r $HADOOP_HOME/ dn1:~/scp -r $HADOOP_HOME/ dn2:~/scp -r $HADOOP_HOME/ dn3:~/ 9. 启动由于我们配置了QJM，所以我们需要先启动QJM的服务，启动顺序如下所示： 启动zookeeper集群。启动完成之后可以输入zkServer.sh status查看启动状态，会出现一个leader和两个follower。输入jps，会显示启动进程：QuorumPeerMain 在NN节点上（选一台即可，这里我选择的是一台预NNA节点），然后启动journalnode服务，命令如下：hadoop-daemons.sh start journalnode。或者单独进入到每个DN输入启动命令：hadoop-daemon.sh start journalnode。输入jps显示启动进程：JournalNode 接着若是配置后首次启动，需要格式化HDFS，命令如下：hadoop namenode –format 之后我们需要格式化ZK，命令如下：hdfs zkfc –formatZK 接着我们启动hdfs，命令如下：start-dfs.sh，我们在nna输入jps查看进程，显示如下：DFSZKFailoverController，NameNode，ResourceManager。DN节点也会自动启动DataNode、NodeManager 接着我们在NNS输入jps查看，发现只有DFSZKFailoverController进程，这里我们需要手动启动NNS上的namenode。命令如下：hadoop-daemon.sh start namenode 最后我们需要同步NNA节点的元数据，命令如下：hdfs namenode –bootstrapStandby 启动yarn: 接着上面的步骤，在NNA节点上：start-yarn.sh。输入jps查看进程，会发现多了：ResourceManager 登录NNS节点，使用：yarn-daemon.sh start resourcemanager。需要注意的是，在NNS上的yarn-site.xml中，需要配置指向NNS，属性配置为rm2，在NNA中配置的是rm1。 启动完成之后，可以访问： hdfs: http://192.168.1.4:50770/dfshealth.html 或 http://192.168.1.5:50770/dfshealth.html （其中一个为“active”,一个为“standby”） resourcemanger: http://192.168.1.4:8188/cluster/cluster 或 http://192.168.1.5:8188/cluster/cluster 测试：1234hadoop fs -mkdir hdfs:/test/ hadoop fs -copyFromLocal /home/hadoop/data/webcount hdfs:/test/ hadoop fs -ls hdfs:/test/ hadoop fs -cat hdfs:/test/webcount HA的切换由于我配置的是自动切换，若NNA节点宕掉，NNS节点会立即由standby状态切换为active状态。若是配置的手动状态，可以输入如下命令进行人工切换： 1hdfs haadmin -failover --forcefence --forceactive nna nns 这条命令的意思是，将nna变成standby，nns变成active。而且手动状态下需要重启服务。 工具脚本copy-config.sh1234567891011#!/bin/shscp /home/hadoop/hadoop/etc/hadoop/* nns:/home/hadoop/hadoop/etc/hadoopscp /home/hadoop/hadoop/etc/hadoop/* dn1:/home/hadoop/hadoop/etc/hadoopscp /home/hadoop/hadoop/etc/hadoop/* dn2:/home/hadoop/hadoop/etc/hadoopscp /home/hadoop/hadoop/etc/hadoop/* dn3:/home/hadoop/hadoop/etc/hadoopscp /home/hadoop/hadoop/libexec/* nns:/home/hadoop/hadoop/libexecscp /home/hadoop/hadoop/libexec/* dn1:/home/hadoop/hadoop/libexecscp /home/hadoop/hadoop/libexec/* dn2:/home/hadoop/hadoop/libexecscp /home/hadoop/hadoop/libexec/* dn3:/home/hadoop/hadoop/libexec]]></content>
      <categories>
        <category>技术笔记</category>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>hdfs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RocketMQ Performance Test]]></title>
    <url>%2F%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0%2FJava%2FRocketMQ-Performance-Test.html</url>
    <content type="text"><![CDATA[测试环境硬件配置4C 4G SSD操作系统centeOS 6.5MQ版本rocketmq-broker-4.2.0-incubating-SNAPSHOT （2017-08-23）测试程序运行机器：Macbook Pro i7 2.3GHz, 16G单Broker场景单broker同步发送（单发送进程），每条message大小：30bytesNameServer与Broker分别在不同机器部署，broker采用异步持久化topic设置： perm: 6 JVM设置： NameServer: -Xms1g -Xmx1g -Xmn512m Broker: -Xms2g -Xmx2g -Xmn1g 发送条数成功率耗时（s)发送速率（条/秒）备注100000100%74.2651346.52写queue数量：4100000100%75.3471327.22写queue数量：8结论成功率非常有保证，调整topic的queue数量并不能提升发送速率。broker的能力并没有完全被发挥出来，CPU非常闲。单broker异步发送（单发送进程），每条message大小：30bytesNameServer与Broker分别在不同机器部署，broker采用异步持久化topic设置： perm: 6 JVM设置： NameServer： -Xms1g -Xmx1g -Xmn512m Broker: -Xms2g -Xmx2g -Xmn1g 发送条数成功率耗时（s)发送速率（条/秒）备注1000000100%33.42029922.20写queue数量：4，每发送37条休眠一毫秒，限制生产者速度不超过37000条/秒100000096.6024%20.87146285.46写queue数量：4，发送端不限速1000000100%33.47729871.25写queue数量：8，每发送37条休眠一毫秒，限制生产者速度不超过37000条/秒100000097.9327%22.96148844.23写queue数量：8，发送端不限速结论吞吐量较大时，会触发broker端的流控限制，会导致消息发送失败，出现异常：Rocketmq:MQBrokerException: CODE: 2 DESC: [TIMEOUT_CLEAN_QUEUE]。通过多次反复测试，发现在保证发送成功率100%的情况下，发送速率最大为29922.20条/秒，测试过程中，测试程序最大上传网速达到15.9MB/s。在成功率100%的情况下，吞吐量是同步发送的超20倍。不限速（允许少量失败）的情况下，吞吐量是同步发送的超34倍。在单发送进程场景下，增加topic的queue数量并不能提升生产速率。发现的问题（或BUG）测试发现producer.shutdown()时，并没有完全等待SendCallBack执行完就退出，可能会导致有部分callback无法接收到。因此在在测试程序异步发送完成之后，还需要添加一个线程休眠，确保全部消息全部发送到broker。单broker消费（单消费进程），每条message大小：30bytesNameServer与Broker分别在不同机器部署topic设置： perm: 6 JVM设置： NameServer： -Xms1g -Xmx1g -Xmn512m Broker: -Xms2g -Xmx2g -Xmn1g consumer设置： consumeThreadMax: 64 pullBatchSize: 32 消费条数成功率耗时（s)消费速率（条/秒）备注3000000100%21.353140495.48读queue数量：83000000100%34.37187282.88读queue数量：4结论RocketMQ的消息消费能力非常高，只要消费端的业务处理足够快。单broker的消费超过7w条每秒，基本上可以说满足大部分的实际业务场景。增加topic的queue数量，能明显提高消费速率。（前提是生产的消息要平均分布在各个queue）从测试环境看，消费高峰每秒下载流量超过31MB，Broker的CPU会占用维持在50%左右，消费过程中CPU的IOWait在某一瞬间会超过20%。单broker消费（双消费进程），每条message大小：30bytesNameServer与Broker分别在不同机器部署topic设置： 读队列数量：4 写队列数量：4 perm: 6 JVM设置： NameServer： -Xms1g -Xmx1g -Xmn512m Broker: -Xms2g -Xmx2g -Xmn1g consumer设置： consumeThreadMax: 64 pullBatchSize: 32 消费条数成功率耗时（s)消费速率（条/秒）消费进程 11000000100%22.54944347.86消费进程21000000100%21.36046796.76结论增加消费者进程并不能提高整体的消费速度，因为消费者本身就是多线程的模式消费。但可以通过增加消费者进程来提升消费端高可用能力，避免消费端的单点故障。多Master Broker场景双Master Broker 同步发送，每条message大小：30bytes三台主机，一台部署NameServer，另外两台各部署Broker mastertopic设置： perm: 6 JVM设置： NameServer: -Xms1g -Xmx1g -Xmn512m Broker: -Xms2g -Xmx2g -Xmn1g 发送条数成功率耗时（s)发送速率（条/秒）备注100000100%66.8451495.99每个broker4个queue结论同步发送速率双Master Borker与单broker基本在一个数量级 双Master Broker 异步发送（单发送进程），每条message大小：30bytes三台主机，一台部署NameServer，另外两台各部署Broker mastertopic设置： perm: 6 JVM设置： NameServer: -Xms1g -Xmx1g -Xmn512m Broker: -Xms2g -Xmx2g -Xmn1g Broker设置： 为提高成功率，将waitTimeMillsInSendQueue的默认值从200调整到1000 发送条数成功率耗时（s)发送速率（条/秒）备注1000000100%20.86247934.04每个broker各4个queue， 每发送70条休眠1毫秒，确保每秒发送数量不超过7w条1000000100%20.61648506.01每个broker各8个queue， 每发送70条休眠1毫秒，确保每秒发送数量不超过7w条100000099.7571%16.02662247.03每个broker各4个queue，生产者不限速100000099.8807%17.10858,382.45每个broker各8个queue，生产者不限速结论在保证成功率100%的情况下，双master broker比单broker的发送速率明显提高，提升超60%。增加topic的queue数量，但发送进程并不能提高发送速率双broker消费（单消费进程），每条message大小：30bytes三台主机，一台部署NameServer，另外两台各部署Broker mastertopic设置： perm: 6 JVM设置： NameServer： -Xms1g -Xmx1g -Xmn512m Broker: -Xms2g -Xmx2g -Xmn1g consumer设置： consumeThreadMax: 64 pullBatchSize: 32 消费条数成功率耗时（s)消费速率（条/秒）备注2000000100%11.170179051.02读queue数量：43000000100%11.979250438.26读queue数量：8结论由于topic在每个broker上都有4个queue，发送的消息基本是均匀分配在两个broker的，因此在消费时能从两个broker分别进行消费，消费速率较单broker的消费提升了近1倍。增大topic的queue数量，明显能提高消费速率。双Master双Slave异步复制，异步发送，每条message大小：30bytes五台主机，一台部署NameServer，两台分别部署broker-a-master、broker-a-slave，两台分别部署broker-b-master、broker-b-slave。topic设置： perm: 6 JVM设置： NameServer: -Xms1g -Xmx1g -Xmn512m Broker: -Xms2g -Xmx2g -Xmn1g Broker设置： master broker设置: brokerRole=ASYNC_MASTER 发送条数成功率耗时（s)发送速率（条/秒）备注100000100%20.75548181.16每个broker各8个queue，每发送70条休眠1毫秒，确保每秒发送数量不超过7w条结论Master与Slave采用异步复制时，基本不影响消息发送的吞吐量。发送100w条消息到broker后（平均分配到broker-a与broker-b），关掉broker-b-master，启动消费者时，如果订阅组之前不存在，则不能从broker-b-slave上进行消费。如果订阅组存在，则能从broker-b-slave上进行消费，但当broker-b-master恢复之后，会重复消费broker-b-master上的消息。发现的问题（或BUG）在slave上进行消费时，消费的offset近保存到了slave上，当master恢复时，slave上的消费offset并没有被同步到master上，master上的offset还是最初在master上的消费位置，因此会导致master故障期间从slave上消费的消息会被重复消费。这应该不是RocketMQ的设计初衷，感觉像是个bug。RocketMQ vs KafkaRocketMQ与Kafka一样，都没有JMS中定义的Queue的存在，只有Topic的存在。但他们都可以基于Topic来实现Queue的功能与特性。RocketMQ可以创建只有1个queue的topic，再结合Cluster的消费方式来实现JMS queue的功能与特性。Kafka则可以创建只有一个partition的topic来实现JMS Queue的功能与特性。RocketMQ中queue的概念可类比为Kafka中的partition，若要实现消息的强顺序消费，RocketMQ需要设置topic只有一个queue，Kafka则需要设置topic只有一个partition。若业务规定只需保证同一订单编号的状态变更的顺序性，RocketMQ可以在发送时通过订单编号与MessageQueueSelector来实现同一编号的订单消息放置在topic的同一个queue中，Kafka则可以将订单编号作为消息的key来确保同一编号的订单消息放置在了同一个partition。table th:nth-of-type(5){width:200px}]]></content>
      <categories>
        <category>技术笔记</category>
        <category>Java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>MQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM内存结构简介]]></title>
    <url>%2F%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0%2FJava%2FJava-Memory-Model.html</url>
    <content type="text"><![CDATA[Java运行时内存结构12345678910111213+-------------------------------------------------+| Runtime Memory Structure Chart || +--------------+ | +----------+ +----------+ || | | | | | | Native | || | MethodArea | | | VM Stack | | Method | || | (PermGen) | | | | | Stack | || +--------------+ | +----------+ +----------+ || +--------------+ | +------------------------+ || | | | | | || | Heap | | | Program Counter | || | | | | Register | || +--------------+ +------------------------+ |+-------------------------------------------------+ 线程共享数据区域 Heap: 堆，是JVM最大的内存区。 Method Area(PermGen): 方法区，存储类、常量、静态变量等数据。在某些JVM的实现中也称持久代、永久代。 JDK8之后，PermGen被元空间（MetaSpace）替代 线程之间相互独立区域 VM Stack: 虚拟机栈 Native Method Stack: 本地方法栈 Program Counter Register: 程序计数器 通过一张图来了解如何通过参数来控制各区域的内存大小: 堆（Heap）对于大多数应用来说，java堆（Heap）是JVM管理的最大一块内存。Java堆是被所有线程共享的一块内存，在虚拟机启动时创建，主要用来存放对象实例。 堆也是垃圾收集器管理的主要区域，从内存回收的角度看，大部分收集器采用分代回收，所以Java的堆可以细分为：新生代、老生代。新生代可以分为Eden空间、From空间、To空间。 如果堆中没有内存完成实例分配，并且也无法扩展时将会抛出OutOfMemoryError异常。 123456789101112131415161718+----------------------------------------+| Heap Structure || +--------------------+ +-------------+ || | Young Generation | | | || | +----------------+ | | | || | | Eden | | | | || | | Space | | | | || | +----------------+ | | | || | +----------------+ | | Old/Tenured | || | | FromSpace | | | Generation | || | | (Survivor1) | | | | || | +----------------+ | | | || | +----------------+ | | | || | | ToSpace | | | | || | | (Survivor2) | | | | || | +----------------+ | | | || +--------------------+ +-------------+ |+----------------------------------------+ 新生代(Young Generation) Eden Space From Space(Survivor1) To Space(Survivor2) 老生代(Old Generation) 相关参数设置： 参数名 描述 默认值 备注 -Xms 堆的初始值，设置示例：-Xms10m 如果没有设置此值，默认值=分配的新生代值+分配的老生代的值。设置的值必须是1KB的倍数，且最小为1MB。等同于-XX:InitialHeapSize。 -Xmx 堆的最大值，设置示例：-Xmx1g 根据运行时系统配置选择。 等同于-XX:MaxHeapSize。设置的值必须是1KB的倍数，且最小为2MB。在最为服务器模式运行时，一般都设置-Xms等于-Xmx。 -Xmn 新生代的最大值，设置示例：-Xmn10m 建议设置新生代的大小为整个堆大小的：1/4至1/2之间。等同于-XX:MaxNewSize。 -XX:NewSize 新生代的初始值 -XX:SurvivorRation 用于设置Eden和其中一个Survivor的比值 默认值为8，表示80%的为Eden，两个Survivor各占10% -XX:MaxTenuringThreshold 对象在新生代存活周期的阈值 在并行收集器中默认为15，CMS收集器中默认为6 最大值15 -XX:+PrintTenuringDistribution 用于在Minor GC时打印Survivor中各个年龄段对象的占用空间大小 -XX:NewRatio 用于设置老生代和新生代的比例 默认为2，即1/3为新生代，2/3为老生代 参数命名有些奇怪，实际计算公式: 比例值=老生代大小/新生代大小 新生代（Young Generation）用来存放新的对象实例，垃圾收集器会频繁的在此区域工作，当新生代的Eden区满了之后，会触发Minor GC或Young GC。因此新生代设置过小会导致频繁的Minor GC或Young GC。如果设置过大，则只会在Full GC时才被执行，这会消耗较长的时间。 为了优化GC的性能，把新生代又细分成了Eden、Survivor1(from)、Survivor2(to)三个区域。 Eden存储新生的对象。一般新创建的对象都会被分配到Eden区中，某些对象会特殊处理。默认Eden占新生代80%的大小。 Eden区满了之后会触发Minor GC。 Survivor新生代中有两个Survivor区，一个标记为From，一个标记为TO，在GC开始时，被标记为TO的空间一定是空的。 在Minor GC或Young GC发生时，Eden区中没有被引用（ref）的对象将被清除，需要存活的对象都会被复制到一个标记为TO的Survivor区中，From区中需要继续存活的对象会根据存活周期来决定去向，如果超过存活的周期来会被移动到老生代中，反之也会被复制到标记为TO的Survivor区中，如果TO被填满，则TO中所有的对象都会被移动到老生代中。GC完成之后，每个对象的生命周期年龄都会被加1，Eden和From都被清空，FROM和TO也会互换角色，上一次的TO变成新的FROM，新的TO又将是一个空的区域。 老生代（Old Generation）存放生命周期长的对象。也称为“老年代”。对老生代的垃圾回收称为Old GC，当老生代满了之后会触发此GC。 注意 Old GC并不等同于Major GC或Full GC，根据不同的GC的实现，它们所指的范围都不一样。 方法区（MethodArea/Perm Genration）方法区称作“非堆（Non-Heap）”，用来存放类对象、常量、静态变量、即时编译后的代码数据。与Heap一样都属于线程共享。 在习惯在HotSpot虚拟机上开发和部署的程序员来说，很多人把它称为“永久代（Permanent Generation）”，平常所说的永久代也是指这个区域。 尽管这个区域被称为永久代，但有些垃圾收集器也会在此区域执行回收，这个区域的回收主要是常量池的回收、以及类型的卸载。JVM规范没对此区域的限制非常宽松，允许不对此区域实现垃圾收集。 当方法区无法满足内存分配需求时，将抛出OutOfMemoryError: PermGen异常。 此空间的调整参数： 参数名 描述 默认值 备注 -XX:PermSize 永久代内存初始值 物理内存的1/64，例如：2G内存的机器初始值为32M -XX:MaxPermSize 永久代内存最大值 物理内存的1/4，例如：2G内存的机器初始值为512M 元空间（MetaSpace）JDK8开始，PermGen被元空间（MetaSpace）替代, PermGen被移除。 其实移除PermGen的工作从JDK7就开始了，但并没有完全移除，譬如类的静态变量、字面量（interned strings）都转移到了java heap中，符号引用转移到了native heap。 元空间的本质与PermGen类似，都是对JVM规范中方法区的实现。最大区别是元空间并不在虚拟机中，而是使用本地内存，因此元空间的大小受本地内存限制。 当方法区无法满足内存分配需求时，将抛出OutOfMemoryError: PermGen异常。 元空间的大小是JVM根据垃圾收集的结果来自动调整的。也可以通过如下参数来调整： 参数名 描述 默认值 备注 -XX:MetaspaceSize 元空间初始值 在默认情况下，这个值大小根据不同的平台在12M到20M浮动 该值越大触发Metaspace GC的时机就越晚，达到该值就会触发垃圾收集进行类型卸载。同时垃圾收集器会对该值进行调整：如果释放了大量空间，就会适当降低该值。如果释放了很少的空间，在不超过MaxMetaspaceSize的时，会适当提高该值。受本机最大可用内存限制，受32位与64位的JVM、操作系统限制 -XX:MaxMetaspaceSize 元空间最大值 无限制 超过最大值时，将抛出OutOfMemoryError: PermGen异常。 -XX:MinMetaspaceFreeRatio 元空间最小空闲占比 NA 当进行过元空间GC之后，如果当前元空间的空闲占比小于此值，则增长元空间的大小。此参数可以控制元空间的增长速度，如果该值过小会导致元空间的增长缓慢，可能会影响之后的类加载，如果该值过大会导致元空间增长过快，浪费内存。本机测试效果来看默认值在40左右，也就是40% -XX:MaxMetaspaceFreeRatio 元空间最大空闲占比 NA 当进行过元空间GC之后，如果当前元空间的空闲占比超过此值，则会释放部分元空间。本机测试效果来看默认值在70左右，也就是70% -XX:MinMetaspaceExpansion 元空间增长时的最小幅度 NA 在本机上该参数的默认值为340784B（大约330KB为） -XX:MaxMetaspaceExpansion 元空间增长时的最大幅度 NA 在本机上该参数的默认值为5452592B（大约为5MB） 为什么要将PermGen切换为Metaspace? 字符串存在永久代中，容易出现性能问题和内存溢出。 类及方法的信息等比较难确定其大小，因此对于永久代的大小指定比较困难，太小容易出现永久代溢出，太大则容易导致老生代溢出。 永久代会为 GC 带来不必要的复杂度，并且回收效率偏低。 Oracle 可能会将HotSpot 与 JRockit 合二为一。 程序计数器（Program Counter）程序计数器也被称为“PC寄存器”。JVM支持多线程同时运行，每个线程都有自己独立且私有的程序计数器，占用空间极少，在线程创建时创建。 解释器通过它来获取下一条的字节码执行指令。如果执行的是java的方法，该程序计数器中保存的是当前执行指令的地址，如果是native方法，则该程序计数器中的值为undefined。 不会有OutOfMemoryError抛出。 栈（Stack）虚拟机栈（VM Stack）虚拟机栈所使用的空间也是线程私有的，以栈帧为单位进行压栈和出栈。 123456789101112131415161718192021222324252627282930313233343536 当前线程 线程2 线程n+-----------------------------+ +------+ +------+| 当前栈帧 | | | | || Current Stack Frame | | | | |+-----------------------------+ | | | || +-------------------------+ | | | | || | 局部变量 | | | | | || | Local Variable Table | | | | | || +-------------------------+ | | | | || +-------------------------+ | | | | || | 操作数栈 | | | | | || | Operand Stack | | | | | || +-------------------------+ | | | | || +-------------------------+ | | | | || | 动态连接 | | | | | || | Dynamic Linking | | | | | || +-------------------------+ | | | | || +-------------------------+ | | | | || | 返回地址 | | | | | || | Return Address | | | | ...... | || +-------------------------+ | | | | || +-------------------------+ | | | | || | 附加信息 | | | | | || | Additional Info | | | | | || +-------------------------+ | | | | || ...... | | | | |+-----------------------------+ | | | || | | | | || Stack Frame n | | | | |+-----------------------------+ | | | || | | | | || Stack Frame 2 | | | | |+-----------------------------+ | | | || | | | | || Stack Frame 1 | | | | |+-----------------------------+ +------+ +------+ 局部变量表 每一个方法都拥有一块属于自己的内存区域来保存方法内部定义的局部变量，这块区域就是局部变量表，当这个方法运行结束后，这个局部变量的生命周期也就宣告结束。我们平常工作中所指的栈，实际上指的是虚拟机栈中的栈帧中的局部变量表。 操作数栈 每个方法的内部都可以计算数据，而计算数据势必需要拥有一块内存区域，为虚拟机用来进行数值计算。因此在栈帧中，就需要有一块区域专门为当前方法计算数据使用，它就是操作数栈。 在每进行一次完整的计算之后，栈中的数据都已经出栈，所以操作数栈的空间在一个方法内部是可以反复使用的。所以虚拟机在分配内存大小时，只分配当前方法，单次完整计算所需要的最大内存空间给当前栈帧，以减少内存的消耗。 同时为了增加运行效率，减少数据的不断复制，在大部分虚拟机的实现中，将当前方法的局部变量表和上层方法的操作数栈的内存形成部分重叠，从而减少参数的不断复制而引起的性能消费。 动态连接 虚拟机在执行方法时有两种形式被用来确定执行指令所对应的方法，第一种是类加载时，可以直接确定要执行的方法，譬如静态方法，私有方法，final方法等。这种形式叫做静态解析。第二种是在真正运行时，根据对象的真实引用来判断当前真正要执行的方法，这种形式称之为动态连接。 在字节码文件中，都存在一个常量池，在这个常量池中保存有大量的符号引用，这个符号引用是每一个方法的间接引用。在字节码指令的中，使用的是这个符号引用。但是在运行时阶段，肯定需要调用到要执行方法在内存中真实的地址。这就需要将间接引用转化成直接引用。而这里的“动态连接”就是为了保证在运行时阶段，方法可以正确的找到要调用的方法，每个栈帧将自己在运行时常量池中所对应的真实地址记录的位置。 这里需要注意的是，在栈帧中的动态连接和查找符号引用为真实引用中的动态连接，是两个概念。前者表示的是一个区域，后者表示的是一种查找方式。 返回地址 退出当前方法的方式有两种，第一种是遇到返回指令时，正常的退出当前方法。另一种形式是遇到没有捕获而被抛出的异常。无论何种返回形式，在方法退出后，栈帧的顶端都应是当前退出方法的上层方法。同时上层方法的执行状态也需要根据当前的返回结果重新调整。所以每个栈帧可以利用“返回地址”这块区域帮助上层方法恢复状态。 附加信息 对于虚拟机规范中没有申明的，拥有指定存放位置的信息可以由各个虚拟机自己决定，放置到这个区域中。 有两种可能的异常抛出：StackOverflowError、OutOfMemoryError。StackOverflowError指的是内存中的栈结构在不断的入栈，最终导致栈的深度超过了虚拟机所允许的栈深度时，所抛出的错误 相关参数设置： 参数名 描述 默认值 备注 -Xss 线程栈大小，设置示例：-Xss320k 不同的平台默认值不同。32位环境一般为320kb，64位环境一般为1024kb。 此参数等同于XX:ThreadStackSize 本地方法栈（Native Method Stack）在虚拟机中，不但运行java方法，还会运行本地方法，也就是常见的native关键字修饰的方法。本地方法运行所使用的空间就是本地方法栈，其也是线程私有的。 它的作用跟虚拟机栈基本相似，其区别就是一个为java方法服务，一个为Native发光法服务。在虚拟机规范中，对于本地方法栈中的结构、方法的语言、方式，都没有强制规定，各个虚拟机可以自由的实现它。 直接内存（Direct Memory）这块内存不属于运行时数据区，所以不受JVM堆大小的限制。 从Jdk1.4开始，NIO（new I/O）变可以直接使用Native函数直接分配这块内存。使用Java堆中的DirectByteBuffer对象作为这块内存的引用。 在使用NIO的应用中，配置虚拟机参数需要考虑到这块内存的大小分配，申请不到内存时也会抛出OutOfMemoryError。 相关参数设置： 参数名 描述 默认值 备注 -XX:MaxDirectMemorySize 最大直接内存值，设置示例：-XX:MaxDirectMemorySize=10m 默认情况下，大小设置为0，这意味着JVM将自动分配和扩展。 关于GC针对HotSpot VM的实现，GC的分类只有两大种： Partial GC: 局部GC Young GC: 只收集新生代 Old GC: 只收集老生代，只有CMS的concurrent collection是这个模式 Mixed GC: 收集所有新生代以及部分老生代。只有G1才有此模式。 Full GC: 全量GC，收集整个堆(包括新生代和老生代)、以及方法区（java8之前的PermGen, java8开始的metaspace）。 通常所说的Major GC与Full GC是等价的。但由于HotSpot VM发展了这么多年，很对名词解读已经混乱，当有人说Major GC时，一点要问清楚他说的是Full GC还是Old GC。 对于HotSpot VM的串行收集器（Serial GC）的实现来看，各GC场景的触发条件是： Young GC: Eden区没有足够空间进行分配时触发； Old GC: 老生代没有足够空间进行分配时触发；只有CMS的concurrent collection是这个模式 Full GC: 方法区PermGen或Metaspace没有足够空间进行分配时触发 在准备触发Young GC时，如果发现之前Young GC移动到老生代的平均大小大于当前老生代剩余空间时，会取消Young GC转而触发Full GC (除CMS的concurrent collection之外，其它的针对老生代的回收一般都会包含对新生代的处理) 程序调用System.gc() 堆HeapDump时带GC，默认也会触发 对于HotSpot VM的并行收集器（Parallel GC）的实现则不一样，以CMS为例，它会定时去检查老生代的是用量，超过一定的比例就会触发。 相关测试代码堆（Heap）内存溢出测试代码1234567891011121314151617181920212223242526import java.util.ArrayList;import java.util.List;/** * 堆（Heap）内存溢出测试代码 * 启动时添加如下参数可以观察GC日志： * -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:-UseCompressedClassPointers * 添加如下参数设置JVM堆大小： * -Xms16m -Xmn8m -Xmx16m */public class HeapTest &#123; public static void main(String[] args) &#123; List&lt;byte[]&gt; list = new ArrayList&lt;byte[]&gt;(); int i = 0; boolean flag = true; while (flag)&#123; try &#123; i++; list.add(new byte[1024 * 1024]);//每次增加一个1M大小的数组对象 &#125;catch (Throwable e)&#123; e.printStackTrace(); flag = false; System.out.println("count=" + i);//记录运行的次数 &#125; &#125; &#125; 方法区（PermGen/Metaspace）内存溢出测试代码123456789101112131415161718192021222324252627282930313233343536373839404142434445import jdk.internal.org.objectweb.asm.ClassWriter;import jdk.internal.org.objectweb.asm.MethodVisitor;import jdk.internal.org.objectweb.asm.Opcodes;import java.util.ArrayList;import java.util.List;/** * 方法区（PermGen/Metaspace）内存溢出测试代码 * 启动时添加如下参数可以观察GC日志： * -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:-UseCompressedClassPointers */public class MetaspaceTest extends ClassLoader &#123; public static void main(String[] args) &#123; // 类持有 List&lt;Class&lt;?&gt;&gt; classes = new ArrayList&lt;&gt;(); // 死循环不断的生成不同的类。 for (int i = 1; i &gt; 0; i++) &#123; ClassWriter cw = new ClassWriter(0); // 定义一个类名称为Class&#123;i&#125;，它的访问域为public，父类为java.lang.Object，不实现任何接口 cw.visit(Opcodes.V1_1, Opcodes.ACC_PUBLIC, "Class" + i, null, "java/lang/Object", null); // 定义构造函数&lt;init&gt;方法 MethodVisitor mw = cw.visitMethod(Opcodes.ACC_PUBLIC, "&lt;init&gt;", "()V", null, null); // 第一个指令为加载this mw.visitVarInsn(Opcodes.ALOAD, 0); // 第二个指令为调用父类Object的构造函数 mw.visitMethodInsn(Opcodes.INVOKESPECIAL, "java/lang/Object", "&lt;init&gt;", "()V", false); // 第三条指令为return mw.visitInsn(Opcodes.RETURN); mw.visitMaxs(1, 1); mw.visitEnd(); MetaspaceTest test = new MetaspaceTest(); byte[] code = cw.toByteArray(); // 定义类 Class&lt;?&gt; exampleClass = test.defineClass("Class" + i, code, 0, code.length); classes.add(exampleClass); &#125; &#125;&#125; 虚拟机栈（VM Stack）溢出测试源码12345678910111213141516171819202122/** * 虚拟机栈（VM Stack）溢出测试源码 * 本机测试大概在栈深度达到22217时会出现溢出，每次运行值存在一定偏差 */public class StackTest &#123; private static int index = 1; public void call()&#123; index++; call(); &#125; public static void main(String[] args) &#123; StackTest mock = new StackTest(); try &#123; mock.call(); &#125;catch (Throwable e)&#123; System.out.println("Stack deep : " + index); e.printStackTrace(); &#125; &#125;&#125; 字符串常量溢出测试源码123456789101112131415161718/** * 字符串常量溢出测试源码. * jvm 6中运行会抛出`OutOfMemoryError: PermGen space` * jvm 7和jvm 8中运行会抛出`OutOfMemoryError: Java heap space` */public class StringTest &#123; static String static_str = "xxxxxx"; public static void main(String[] args) &#123; List&lt;String&gt; list = new ArrayList&lt;String&gt;(); // 以2的指数级不断的生成新的字符串 while(true)&#123; String str = static_str + static_str; base = static_str; list.add(str.intern()); &#125; &#125;&#125; 参考资料 https://docs.oracle.com/javase/8/docs/technotes/tools/unix/java.html http://www.importnew.com/23746.html http://blog.csdn.net/xlnjulp/article/details/46763045 https://dzone.com/articles/java-8-permgen-metaspace http://caoyaojun1988-163-com.iteye.com/blog/1969853 http://www.cnblogs.com/smyhvae/p/4748392.html]]></content>
      <categories>
        <category>技术笔记</category>
        <category>Java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>jvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hystrix 配置属性参考]]></title>
    <url>%2F%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0%2FJava%2FHystrix-configuration.html</url>
    <content type="text"><![CDATA[介绍Hystrix使用Archaius作为配置属性的默认实现。下面的文档描述了默认使用的HystrixPropertiesStrategy实现，你也可以使用插件的方式来覆盖它。每个属性有四个优先级：代码的全局默认值如果没有设置以下3个，则这是默认值。 全局默认值在下表中显示为 “默认值”。动态全局默认属性你可以使用属性更改全局默认值。 全局默认属性名称在下表中显示为 “默认属性”。用代码定义默认实例属性你可以定义特定于实例的默认值。例：1HystrixCommandProperties.Setter().withExecutionTimeoutInMilliseconds(int value) 你需要类似于以下的方式将这种命令插入到HystrixCommand构造函数中： 12345public HystrixCommandInstance(int id) &#123; super(Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey("ExampleGroup")) .andCommandPropertiesDefaults(HystrixCommandProperties.Setter().withExecutionTimeoutInMilliseconds(500))); this.id = id;&#125; 一般的设置初始值可以采用便利的构造函数方式。这是一个例子： 1234public HystrixCommandInstance(int id) &#123; super(HystrixCommandGroupKey.Factory.asKey("ExampleGroup"), 500); this.id = id;&#125; 动态设置实例属性 你可以动态设置实例特定的值，从而覆盖前面三个默认级别。 动态实例属性名称在下表中显示为 “实例属性”。 例： 实例属性 hystrix.command.HystrixCommandKey.execution.isolation.thread.timeoutInMilliseconds 将属性的HystrixCommandKey部分替换为您所定位的HystrixCommand的HystrixCommandKey.name())值。 例如，如果被命名为“SubscriberGetAccount”，则属性名称将是： hystrix.command.SubscriberGetAccount.execution.isolation.thread.timeoutInMilliseconds 命令属性（Command Properties）以下属性将控制HystrixCommand的行为： 执行（Execution）以下属性控制HystrixCommand.run())执行。 execution.isolation.strategy这个属性指示HystrixCommand.run()执行的隔离策略，有以下两种选择之一： THREAD —— 它在单独的线程上执行，并发请求受线程池中线程数量的限制 SEMAPHORE —— 它在调用线程上执行，并发请求受信号计数的限制 线程或信号量 缺省值和建议的设置是运行HystrixCommand时使用线程隔离（THREAD），和运行HystrixObservableCommand时使用信号隔离（SEMAPHORE）。 在线程中执行命令能对网络超时提供另一层保护。 通常情况下，对于HystrixCommand来说，唯一使用信号量隔离的场景是，当调用量非常大（每秒数百次）时单独线程的开销太高；这通常只适用于非网络调用。 Netflix的API在40+线程池中运行100+命令，并且只有少数命令不在线程中运行 —— 那些从内存中的缓存提取元数据或或是门面到线程隔离命令（更多信息请参见Primary + Secondary with Fallback” pattern）。 更多信息请参见隔离是如何工作的。 默认值 THREAD（参见：ExecutionIsolationStrategy.THREAD） 取值范围 THREAD, SEMAPHORE 默认属性 hystrix.command.default.execution.isolation.strategy 实例属性 hystrix.command.HystrixCommandKey.execution.isolation.strategy 如何设置默认实例 // to use thread isolation HystrixCommandProperties.Setter().withExecutionIsolationStrategy(ExecutionIsolationStrategy.THREAD) // to use semaphore isolation HystrixCommandProperties.Setter().withExecutionIsolationStrategy(ExecutionIsolationStrategy.SEMAPHORE) execution.isolation.thread.timeoutInMilliseconds该属性设置以毫秒为单位，在该时间之后，调用者将观察到超时并离开命令执行。 Hystrix将HystrixCommand标记为TIMEOUT，并执行回退逻辑。请注意，如果需要可以为每个命令配置关闭超时的设置（请参阅command.timeout.enabled）。 注意： 超时将在HystrixCommand.queue()上触发，即使调用者从未在生成的Future上调用get()。在Hystrix 1.4.0之前，只有调用get()才能使超时机制在这种情况下生效。 默认值 1000 默认属性 hystrix.command.default.execution.isolation.thread.timeoutInMilliseconds 实例属性 hystrix.command.HystrixCommandKey.execution.isolation.thread.timeoutInMilliseconds 如何设置默认实例 HystrixCommandProperties.Setter().withExecutionTimeoutInMilliseconds(int value) execution.timeout.enabled该属性指示HystrixCommand.run()执行是否应该有一个超时。 默认值 true 默认属性 hystrix.command.default.execution.timeout.enabled 实例属性 hystrix.command.HystrixCommandKey.execution.timeout.enabled 如何设置默认实例 HystrixCommandProperties.Setter().withExecutionTimeoutEnabled(boolean value) execution.isolation.thread.interruptOnTimeout这个属性指示HystrixCommand.run()执行是否应该在发生超时时被中断。 默认值 true 默认属性 hystrix.command.default.execution.isolation.thread.interruptOnTimeout 实例属性 hystrix.command.HystrixCommandKey.execution.isolation.thread.interruptOnTimeout 如何设置默认实例 HystrixCommandProperties.Setter().withExecutionIsolationThreadInterruptOnTimeout(boolean value) execution.isolation.thread.interruptOnCancel这个属性指示HystrixCommand.run()执行是否应该在发生取消时被中断。 默认值 false 默认属性 hystrix.command.default.execution.isolation.thread.interruptOnCancel 实例属性 hystrix.command.HystrixCommandKey.execution.isolation.thread.interruptOnCancel 如何设置默认实例 HystrixCommandProperties.Setter().withExecutionIsolationThreadInterruptOnCancel(boolean value) execution.isolation.semaphore.maxConcurrentRequests当您使用ExecutionIsolationStrategy.SEMAPHORE时，此属性设置允许HystrixCommand.run()方法的最大请求数。 如果达到这个最大并发限制，则后续请求将被拒绝。 当你选择一个信号量时，你使用的逻辑基本上和你选择线程池中添加多少个线程相同，但是信号量的开销要小得多，通常执行速度要快得多（亚毫秒） ，否则你会使用线程。 例如，5000rps的单个实例在内存中查找指标聚集仅需要2个信号量就能工作。 隔离原理仍然是相同的，所以信号量应该仍然是整个容器（如：Tomcat）线程池的一小部分，而不是全部或大部分，否则它不提供保护。 默认值 10 默认属性 hystrix.command.default.execution.isolation.semaphore.maxConcurrentRequests 实例属性 hystrix.command.HystrixCommandKey.execution.isolation.semaphore.maxConcurrentRequests 如何设置默认实例 HystrixCommandProperties.Setter().withExecutionIsolationSemaphoreMaxConcurrentRequests(int value) 回退（Fallback）以下属性控制HystrixCommand.getFallback()如何执行。这些属性适用于ExecutionIsolationStrategy.THREAD和ExecutionIsolationStrategy.SEMAPHORE。 fallback.isolation.semaphore.maxConcurrentRequests该属性设置HystrixCommand.getFallback()方法允许从调用线程中创建的最大请求数。 如果达到最大并发限制，则随后的请求将被拒绝并抛出异常，因为没有回退被提取到。 默认值 10 默认属性 hystrix.command.default.fallback.isolation.semaphore.maxConcurrentRequests 实例属性 hystrix.command.HystrixCommandKey.fallback.isolation.semaphore.maxConcurrentRequests 如何设置默认实例 HystrixCommandProperties.Setter().withFallbackIsolationSemaphoreMaxConcurrentRequests(int value) fallback.enabledSince: 1.2 该属性设置是否在发生故障或拒绝时尝试调用HystrixCommand.getFallback()。 默认值 true 默认属性 hystrix.command.default.fallback.enabled 实例属性 hystrix.command.HystrixCommandKey.fallback.enabled 如何设置默认实例 HystrixCommandProperties.Setter().withFallbackEnabled(boolean value) 断路器（Circuit Breaker）断路器属性控制HystrixCircuitBreaker的行为。 circuitBreaker.enabled该属性设置是否将使用断路器来跟踪健康状况并且如果断路器跳闸则将其短路。 默认值 true 默认属性 hystrix.command.default.circuitBreaker.enabled 实例属性 hystrix.command.HystrixCommandKey.circuitBreaker.enabled 如何设置默认实例 HystrixCommandProperties.Setter().withCircuitBreakerEnabled(boolean value) circuitBreaker.requestVolumeThreshold该属性设置滚动窗口中将使电路跳闸的最小请求数量。 例如，如果值是20，那么如果在滚动窗口中接收到19个请求（例如10秒的窗口），则即使所有19个请求都失败，电路也不会跳闸。 默认值 20 默认属性 hystrix.command.default.circuitBreaker.requestVolumeThreshold 实例属性 hystrix.command.HystrixCommandKey.circuitBreaker.requestVolumeThreshold 如何设置默认实例 HystrixCommandProperties.Setter().withCircuitBreakerRequestVolumeThreshold(int value) circuitBreaker.sleepWindowInMilliseconds该属性设置跳闸后的时间量，拒绝请求，然后再次尝试确定电路是否应再次闭合。 默认值 5000 默认属性 hystrix.command.default.circuitBreaker.sleepWindowInMilliseconds 实例属性 hystrix.command.HystrixCommandKey.circuitBreaker.sleepWindowInMilliseconds 如何设置默认实例 HystrixCommandProperties.Setter().withCircuitBreakerSleepWindowInMilliseconds(int value) circuitBreaker.errorThresholdPercentage该属性设置错误百分比，在该值以上，电路应断开并开始将请求短路到回退逻辑。 默认值 50 默认属性 hystrix.command.default.circuitBreaker.errorThresholdPercentage 实例属性 hystrix.command.HystrixCommandKey.circuitBreaker.errorThresholdPercentage 如何设置默认实例 HystrixCommandProperties.Setter().withCircuitBreakerErrorThresholdPercentage(int value) circuitBreaker.forceOpen如果该属性为真，则强制断路器进入打开（跳闸）状态，将拒绝所有的请求。 此属性优先于circuitBreaker.forceClosed。 默认值 false 默认属性 hystrix.command.default.circuitBreaker.forceOpen 实例属性 hystrix.command.HystrixCommandKey.circuitBreaker.forceOpen 如何设置默认实例 HystrixCommandProperties.Setter().withCircuitBreakerForceOpen(boolean value) circuitBreaker.forceClosed如果该属性为真，则强制断路器进入关闭状态，在该状态下将允许请求，而不管错误百分比如何。 circuitBreaker.forceOpen属性优先，所以如果它被设置为true，这个属性什么都不做。 默认值 false 默认属性 hystrix.command.default.circuitBreaker.forceClosed 实例属性 hystrix.command.HystrixCommandKey.circuitBreaker.forceClosed 如何设置默认实例 HystrixCommandProperties.Setter().withCircuitBreakerForceClosed(boolean value) 度量（Mitrics）以下属性与从HystrixCommand和HystrixObservableCommand执行捕获指标有关。 metrics.rollingStats.timeInMilliseconds该属性设置统计滚动窗口的持续时间，以毫秒为单位。这是Hystrix保持断路器使用和发布指标的时间。 从1.4.12开始，此属性仅影响初始度量标准的创建，启动后对此属性所做的调整将不会生效。这样可以避免指标数据丢失，也可以优化指标收集。 窗口根据这些增量被分成桶和“卷”。 例如，如果将此属性设置为10秒（10000），具有十个1秒桶的，则下图显示了如何将新桶和旧桶关闭： 默认值 10000 默认属性 hystrix.command.default.metrics.rollingStats.timeInMilliseconds 实例属性 hystrix.command.HystrixCommandKey.metrics.rollingStats.timeInMilliseconds 如何设置默认实例 HystrixCommandProperties.Setter().withMetricsRollingStatisticalWindowInMilliseconds(int value) metrics.rollingStats.numBuckets该属性设置滚动统计窗口分成的桶的数量。 注意： 必须要确保如下条件： “metrics.rollingStats.timeInMilliseconds ％ metrics.rollingStats.numBuckets == 0” ， 否则会引发异常。 换句话说，10000/10是可以的，10000/20也行，但是10000/7不行。 从1.4.12开始，此属性仅影响初始度量标准的创建，启动后对此属性所做的调整将不会生效。这样可以避免指标数据丢失，也可以优化指标收集。 默认值 10 取值范围 可以被metric.rollingStats.timeInMilliseconds整除的任何值。结果应该是数百或数千毫秒的桶。大容量的性能还没有测试过小于100ms的桶。 默认属性 hystrix.command.default.metrics.rollingStats.numBuckets 实例属性 hystrix.command.HystrixCommandKey.metrics.rollingStats.numBuckets 如何设置默认实例 HystrixCommandProperties.Setter().withMetricsRollingStatisticalWindowBuckets(int value) metrics.rollingPercentile.enabled这个属性表示执行延迟是否应该跟踪和计算为百分比。如果他们被禁用，则所有汇总统计（平均值，百分位数）返回为-1。 默认值 true 默认属性 hystrix.command.default.metrics.rollingPercentile.enabled 实例属性 hystrix.command.HystrixCommandKey.metrics.rollingPercentile.enabled 如何设置默认实例 HystrixCommandProperties.Setter().withMetricsRollingPercentileEnabled(boolean value) metrics.rollingPercentile.timeInMilliseconds此属性设置滚动窗口的持续时间，在该窗口中保留执行时间以允许百分数计算（以毫秒为单位）。 窗口根据这些增量被分成桶和“卷”。 从1.4.12开始，此属性仅影响初始度量标准的创建，启动后对此属性所做的调整将不会生效。这样可以避免指标数据丢失，也可以优化指标收集。 默认值 6000 默认属性 hystrix.command.default.metrics.rollingPercentile.timeInMilliseconds 实例属性 hystrix.command.HystrixCommandKey.metrics.rollingPercentile.timeInMilliseconds 如何设置默认实例 HystrixCommandProperties.Setter().withMetricsRollingPercentileWindowInMilliseconds(int value) metrics.rollingPercentile.numBuckets这个属性设置了rollingPercentile窗口将被分成的桶的数量。 注意： 必须要确保如下条件：“metrics.rollingPercentile.timeInMilliseconds ％ metrics.rollingPercentile.numBuckets == 0” ，否则会引发异常。 换句话说，60000/6是可以的，60000/60也是可以的，但10000/7不行。 从1.4.12开始，此属性仅影响初始度量标准的创建，启动后对此属性所做的调整将不会生效。这样可以避免指标数据丢失，也可以优化指标收集。 默认值 6 取值范围 可以被metric.rollingPercentile.timeInMilliseconds整除的任何值。结果应该是数千毫秒的桶。大容量的性能还没有测试过小于1000ms的桶。 默认属性 hystrix.command.default.metrics.rollingPercentile.numBuckets 实例属性 hystrix.command.HystrixCommandKey.metrics.rollingPercentile.numBuckets 如何设置默认实例 HystrixCommandProperties.Setter().withMetricsRollingPercentileWindowBuckets(int value) metrics.rollingPercentile.bucketSize该属性设置每个桶的最大执行次数。如果更多的执行在此期间发生，他们将环绕并开始在桶的开头重写。 例如，如果桶大小设置为100，并桶的窗口为10秒，但是在此期间发生500次执行，则只有最后100次执行将保留在该10秒的桶中。 如果增加这个大小，这也增加了存储值所需的内存量，并增加了对列表进行排序以进行百分比计算所需的时间。 从1.4.12开始，此属性仅影响初始度量标准的创建，启动后对此属性所做的调整将不会生效。这样可以避免指标数据丢失，也可以优化指标收集。 默认值 100 默认属性 hystrix.command.default.metrics.rollingPercentile.bucketSize 实例属性 hystrix.command.HystrixCommandKey.metrics.rollingPercentile.bucketSize 如何设置默认实例 HystrixCommandProperties.Setter().withMetricsRollingPercentileBucketSize(int value) metrics.healthSnapshot.intervalInMilliseconds此属性设置允许执行运行成功和错误百分比并影响断路器状态的健康快照之间等待的时间（以毫秒为单位）。 在大容量的电路上，连续计算误差百分比可能会成为CPU密集型的，因此这个属性允许你控制计算的频率。 默认值 500 默认属性 hystrix.command.default.metrics.healthSnapshot.intervalInMilliseconds 实例属性 hystrix.command.HystrixCommandKey.metrics.healthSnapshot.intervalInMilliseconds 如何设置默认实例 HystrixCommandProperties.Setter().withMetricsHealthSnapshotIntervalInMilliseconds(int value) 请求上下文（Request Context）这些属性涉及HystrixCommand使用HystrixRequestContext功能。 requestCache.enabled此属性指示HystrixCommand.getCacheKey())是否应与HystrixRequestCache一起使用，以通过请求范围缓存提供重复数据删除功能。 默认值 true 默认属性 hystrix.command.default.requestCache.enabled 实例属性 hystrix.command.HystrixCommandKey.requestCache.enabled 如何设置默认实例 HystrixCommandProperties.Setter().withRequestCacheEnabled(boolean value) requestLog.enabled此属性指示是否应将HystrixCommand执行和事件记录到HystrixRequestLog 默认值 true 默认属性 hystrix.command.default.requestLog.enabled 实例属性 hystrix.command.HystrixCommandKey.requestLog.enabled 如何设置默认实例 HystrixCommandProperties.Setter().withRequestLogEnabled(boolean value) 破裂器属性（Collapser Properties）以下属性控制HystrixCollapser行为。 maxRequestsInBatch此属性设置批处理允许的最大请求数量，然后触发批处理执行。 默认值 Integer.MAX_VALUE 默认属性 hystrix.collapser.default.maxRequestsInBatch 实例属性 hystrix.collapser.HystrixCollapserKey.maxRequestsInBatch 如何设置默认实例 HystrixCollapserProperties.Setter().withMaxRequestsInBatch(int value) timerDelayInMilliseconds此属性设置触发执行之后创建批处理的毫秒数。 默认值 10 默认属性 hystrix.collapser.default.timerDelayInMilliseconds 实例属性 hystrix.collapser.HystrixCollapserKey.timerDelayInMilliseconds 如何设置默认实例 HystrixCollapserProperties.Setter().withTimerDelayInMilliseconds(int value) requestCache.enabled此属性指示是否为HystrixCollapser.execute()和HystrixCollapser.queue()调用启用了请求缓存。 默认值 true 默认属性 hystrix.collapser.default.requestCache.enabled 实例属性 hystrix.collapser.HystrixCommandKey.requestCache.enabled 如何设置默认实例 HystrixCollapserProperties.Setter().withRequestCacheEnabled(boolean value) 线程池属性（Thread Pool Properties）以下属性控制Hystrix命令执行的线程池的行为。请注意，这些名称与ThreadPoolExecutor Javadoc中的名称相匹配 大多数情况下，10个线程的默认值都可以（通常可以做得更小）。 要确定是否需要更大，计算大小的基本公式是： 在健康时每秒的请求高峰数 × 99%的延迟秒数+一些喘息的空间时。 看下面的例子，看看这个公式是如何实施的。 总体原则是尽可能保持池的小，因为它是减轻负载并防止资源在延迟发生时被阻塞的主要工具。 Netflix API有30+的线程池被设置为10，两个在20，一个在25。 上图显示了一个配置示例，其中依赖关系没有理由达到第99.5百分位，因此它在网络超时层将其缩短，并立即重试，并期望大部分时间会得到中位延迟能够在300ms线程超时内完成这一切。 如果依赖有合法的理由有时达到99.5％（比如缓存未命中），那么网络超时将被设置得比它高，比如325ms，重试0或1次，线程超时设置更高（350ms + ）。 线程池的大小为10，以处理第99个百分点请求的突发，但是当一切正常时，此线程池通常在任何给定时间通常只有1或2个线程处于活动状态，以服务大多数40ms的中间调用。 当你正确地配置它时，HystrixCommand层的超时应该是很少的，但是如果网络等待时间以外的事情影响了时间，或者在最坏的情况下连接+读+重试+连接+读超过配置的整体超时。 每个方向的配置和折衷的攻击性对于每个依赖性是不同的。 当性能特征发生变化或发现问题时，您可以根据需要实时更改配置，而且如果出现问题或配置错误，则无需关闭整个应用程序。 coreSize此属性设置核心线程数。 默认值 10 默认属性 hystrix.threadpool.default.coreSize 实例属性 hystrix.threadpool.HystrixThreadPoolKey.coreSize 如何设置默认实例 HystrixThreadPoolProperties.Setter().withCoreSize(int value) maximumSize在1.5.9中添加。该属性设置最大的线程池大小。这是开始无需拒绝HystrixCommand即可支持的最大并发数量。请注意，如果您还设置了allowMaximumSizeToDivergeFromCoreSize，则此设置才会生效。在1.5.9之前，核心和最大大小总是相等的。 默认值 10 默认属性 hystrix.threadpool.default.maximumSize 实例属性 hystrix.threadpool.HystrixThreadPoolKey.maximumSize 如何设置默认实例 HystrixThreadPoolProperties.Setter().withMaximumSize(int value) maxQueueSize该属性设置BlockingQueue实现的最大队列大小。 如果将其设置为-1，则将使用SynchronousQueue，其它正值将使用LinkedBlockingQueue。 注意： 这个属性只适用于初始化时间，因为如果不重新初始化线程执行器，不支持的队列被调整或改变。 如果您需要越过此限制并允许在队列中进行动态更改，请参阅queueSizeRejectionThreshold属性。 要在SynchronousQueue和LinkedBlockingQueue之间切换，需要重新启动。 默认值 -1 默认属性 hystrix.threadpool.default.maxQueueSize 实例属性 hystrix.threadpool.HystrixThreadPoolKey.maxQueueSize 如何设置默认实例 HystrixThreadPoolProperties.Setter().withMaxQueueSize(int value) queueSizeRejectionThreshold此属性设置队列大小拒绝阈值 —— 即使maxQueueSize尚未达到，拒绝将发生的人为最大队列大小。此属性的存在是因为BlockingQueue的maxQueueSize不能动态更改，我们希望允许您动态更改影响拒绝的队列大小。 当排队一个线程执行时，HystrixCommand使用它。 注意： 如果maxQueueSize == -1，则此属性不适用。 默认值 5 默认属性 hystrix.threadpool.default.queueSizeRejectionThreshold 实例属性 hystrix.threadpool.HystrixThreadPoolKey.queueSizeRejectionThreshold 如何设置默认实例 HystrixThreadPoolProperties.Setter().withQueueSizeRejectionThreshold(int value) keepAliveTimeMinutes该属性设置保持活动时间，以分钟为单位。 在1.5.9之前，所有线程池都是固定大小的，如coreSize == maximumSize。在1.5.9之后，将allowMaximumSizeToDivergeFromCoreSize设置为true允许这两个值发散，以便线程池可以获取/释放线程。如果coreSize &lt;maximumSize，那么这个属性控制一个线程在被释放之前将不被使用的时间。 默认值 1 默认属性 hystrix.threadpool.default.keepAliveTimeMinutes 实例属性 hystrix.threadpool.HystrixThreadPoolKey.keepAliveTimeMinutes 如何设置默认实例 HystrixThreadPoolProperties.Setter().withKeepAliveTimeMinutes(int value) allowMaximumSizeToDivergeFromCoreSize在1.5.9中添加。此属性允许maximumSize的配置生效。该值可以等于或高于coreSize。设置coreSize &lt;maximumSize会创建一个线程池，它可以支持maximumSize并发性，但是会在相对不活动的时间内将线程返回给系统。 （以keepAliveTimeInMinutes为准） 默认值 false 默认属性 hystrix.threadpool.default.allowMaximumSizeToDivergeFromCoreSize 实例属性 hystrix.threadpool.HystrixThreadPoolKey.allowMaximumSizeToDivergeFromCoreSize 如何设置默认实例 HystrixThreadPoolProperties.Setter().withAllowMaximumSizeToDivergeFromCoreSize(boolean value) metrics.rollingStats.timeInMilliseconds该属性设置统计滚动窗口的持续时间，以毫秒为单位。这是为线程池保留多长时间的指标。 窗口根据这些增量被分成桶和“卷”。 默认值 10000 默认属性 hystrix.threadpool.default..metrics.rollingStats.timeInMilliseconds 实例属性 hystrix.threadpool.HystrixThreadPoolKey.metrics.rollingStats.timeInMilliseconds 如何设置默认实例 HystrixThreadPoolProperties.Setter().withMetricsRollingStatisticalWindowInMilliseconds(int value) metrics.rollingStats.numBuckets该属性设置滚动统计窗口分成的桶的数量。 注意： 必须要确保如下条件：“metrics.rollingStats.timeInMilliseconds ％ metrics.rollingStats.numBuckets == 0” ，否则会引发异常。 换句话说，10000/10是可以的，10000/20也可以，但是10000/7不行。 默认值 10 取值范围 能被metrics.rollingStats.timeInMilliseconds整除的任何值，结果应该是数百或数千毫秒的桶。大容量的性能还没有测试过小于100ms的桶。 默认属性 hystrix.threadpool.default.metrics.rollingStats.numBuckets 实例属性 hystrix.threadpool.HystrixThreadPoolKey.metrics.rollingStats.numBuckets 如何设置默认实例 HystrixThreadPoolProperties.Setter().withMetricsRollingStatisticalWindowBuckets(int value) 参考资料https://github.com/Netflix/Hystrix/wiki/Configuration table th:first-of-type{width:80px}]]></content>
      <categories>
        <category>技术笔记</category>
        <category>Java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>微服务</tag>
        <tag>hystrix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MacOS下基于Python2.7安装OpenCV3]]></title>
    <url>%2F%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0%2F%E5%B7%A5%E5%85%B7%E7%B1%BB%2FMacOs-install-opencv.html</url>
    <content type="text"><![CDATA[步骤概览安装Xcode以及Apple Command Line Tools安装Homebrew创建Python虚拟环境安装NumPy安装OpenCV安装Xcode以及Apple Command Line Tools从App Store下载并安装Xcode。接受Apple开发者协议。1sudo xcodebuild -license 安装Apple Command Line Tools 1sudo xcode-select --install 输入上面的命令后，会弹出一个窗口，确认你想要安装命令行工具，点击install或安装按钮进行安装。 安装HomebrewHomebrew是MacOS的包管理器，类似于Ubuntu的apt-get。 安装命令： 1/usr/bin/ruby -e "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)" 一旦安装了Homebrew，您应该对其进行更新，以确保下载最新的软件包定义： 1brew update 修改环境变量文件，如果你用的是bash请球盖vi ~/.bash_profile，如果你用的是zsh请修改vi ~/.zshrc，添加如下内容到对应的文件： 12# Homebrewexport PATH=/usr/local/bin:$PATH 使用source命名来让配置生效：source ~/.bash_profile 或 source ~/.zshrc。 创建Python虚拟环境 MacOS默认已安装Python，不同的MacOS版本可能对应的Python版本不同。可以使用python --version来获取当前系统中的Python版本。如果当前版本不是2.7.x，可以使用Homebrew来安装： 1brew install python 安装完成之后，我们需要把操作系统的python链接到Homebrew的安装目录： 1brew linkapps python 检查安装是否正确： 1which python 如果输出的路径是/usr/local/bin/python则说明安装Python成功。 安装虚拟环境 由于在OS X El Capitan中，在内核下引入了Rootless机制，即使root用户也无法对/System、/bin、/sbin、/usr(except /usr/local)目录有写和执行权限，只有Apple以及Apple授权签名的软件（包括命令行工具）可以修改此目录。 因此在使用操作系统自带的Python时，使用pip安装某些包时会存在失败的情况。所以我们在这里使用python虚拟环境来规避这个问题。 1pip install virtualenv virtualenvwrapper 安装完成之后，我们需要再次修改环境变量文件，如果你用的是bash请球盖vi ~/.bash_profile，如果你用的是zsh请修改vi ~/.zshrc，添加如下内容到对应的文件： 12# Virtualenv/VirtualenvWrappersource /usr/local/bin/virtualenvwrapper.sh 使用source命名来让配置生效：source ~/.bash_profile 或 source ~/.zshrc。 创建虚拟环境 1mkvirtualenv opencv 上面命名创建了一个名叫opencv的虚拟环境，你可以使用workon opencv进入虚拟环境，也可以使用deactivate指令从虚拟环境中退出。 安装NumPy 进入虚拟环境: workon opencv pip install numpy 安装OpenCV 环境准备： 123brew install cmake pkg-configbrew install jpeg libpng libtiff openexrbrew install eigen tbb 从github下载OpenCV3 123cd ~git clone https://github.com/opencv/opencvgit clone https://github.com/opencv/opencv_contrib 下载完成之后，进入源码目录切换到指定的版本，譬如安装OpenCV的3.3.1版本： 1234cd ~/opencvgit checkout -b 3.3.1 3.3.1cd ~/opencv_contribgit checkout -b 3.3.1 3.3.1 配置CMake 123cd ~/opencvmkdir buildcd build 接着我们就可以执行cmake命令了，但在执行命令之前有两个参数需要调整： 1234567891011cmake -D CMAKE_BUILD_TYPE=RELEASE \-D CMAKE_INSTALL_PREFIX=/usr/local \-D OPENCV_EXTRA_MODULES_PATH=~/opencv_contrib/modules \-D PYTHON2_LIBRARY=YYY \-D PYTHON2_INCLUDE_DIR=ZZZ \-D PYTHON2_EXECUTABLE=/usr/local/bin/python \-D BUILD_opencv_python2=ON \-D BUILD_opencv_python3=OFF \-D INSTALL_PYTHON_EXAMPLES=ON \-D INSTALL_C_EXAMPLES=OFF \-D BUILD_EXAMPLES=ON .. cmake重要参数解释： BUILD_opencv_python2=ON：这表明我们要基于Python2.7来安装OpenCV 3； BUILD_opencv_python3=OFF：这表明我们关闭Python3； PYTHON2_LIBRARY=YYY：这是你自己填写的第一个值。您将需要用您的libpython2.7.dylib文件的路径替换YYY（我将帮助您在下一节中找到它）； PYTHON2_INCLUDE_DIR=ZZZ：这是您需要填写的第二个值。您需要将ZZZ替换为Python.h头文件所在目录的路径（同样，我将帮助您确定此路径） 获取PYTHON2_LIBRARY的路径 1ls /usr/local/Cellar/python/2.7.*/Frameworks/Python.framework/Versions/2.7/lib/python2.7/config/libpython2.7.dylib 将上面的输出拷贝出来替换掉PYTHON2_LIBRARY=YYY中的YYY。 获取PYTHON2_INCLUDE_DIR的路径 1ls -d /usr/local/Cellar/python/2.7.*/Frameworks/Python.framework/Versions/2.7/include/python2.7/ 将上面的输出拷贝出来替换掉PYTHON2_INCLUDE_DIR=ZZZ中的ZZZ。 替换完成之后即可执行。 编译与安装 1make -j4 -j开关控制并发编译OpenCV的处理器数量，因为我在四核系统上，所以我用-j4。编译可能需要几十分钟左右。 如果编译没有出错就可以进行安装了 1sudo make install 安装成功后，可以在/usr/local/lib/python2.7/site-packges/下找到cv2.so文件。 在虚拟环境中添加cv2.so的软连接 12cd ~/.virtualenvs/cv/lib/python2.7/site-packages/ln -s /usr/local/lib/python2.7/site-packages/cv2.so cv2.so cv2.so 测试与验证OpenCV 打开一个新的终端： 123456789$ workon opencv$ pythonPython 2.7.12 (default, Oct 11 2016, 05:20:59) [GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.38)] on darwinType "help", "copyright", "credits" or "license" for more information.&gt;&gt;&gt; import cv2&gt;&gt;&gt; cv2.__version__'3.3.1&gt;&gt;&gt; 如何完整的卸载OpenCV? 可以通过进入之前的build目录，执行make uninstall来完整卸载OpenCV. 参考资料 MacOS基于Python2.7安装OpenCV3 MacOS基于Python3.5安装OpenCV3]]></content>
      <categories>
        <category>技术笔记</category>
        <category>工具类</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>opencv</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Geetest拖拽验证码破解思路（java）]]></title>
    <url>%2F%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0%2F%E5%B7%A5%E5%85%B7%E7%B1%BB%2FGeetest-Crack.html</url>
    <content type="text"><![CDATA[项目中有个需求希望能获取公司的工商注册信息，刚开始是想与第三方数据公司合作，因种 种原因合作没有达成。于是想做个爬虫直接从工商局的《企业信用信息公示系统》中获取。 要想从《企业信用信息公示系统》中爬取数据，首先必须解决掉Geetest验证码的问题。 经过一系列的摸索，发现要想破解geetest的验证码，主要需要解决如下几个问题：背景图的还原找到背景图中缺口的位置将滑块拖拽到缺口背景图的还原Geetest背景图分为两张，一张是完整背景图，一张是带缺口的背景图。每张图片被分成 52 份， 上下两部分各 26 份，然后乱序排列。在网页上显示时，是通过css来将乱序的 碎片重新组列成完整的图片。乱序图片：正常图片：因此，我们必须将原始的背景乱序图片切根据页面上的css样式将图片切割成 52 份，并 按照先后顺序拼接还原成正确的图片。需要使用到java.awt.image.BufferedImage类：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849/** * 将Geetest打乱的图片还原，Geetest的原始背景图是分成52份碎片乱序组合的。目前是上下各26份，每份碎片图片宽10px， 高58px。 * * @param image 原始背景图 * @param locations 展示位置列表，数据结构：[&#123;x=-25, y=-58&#125;, ...] * @return 顺序排列好的图片 */public static BufferedImage recover(BufferedImage image, List&lt;Map&lt;String, Integer&gt;&gt; locations) throws IOException &#123; long begin = System.currentTimeMillis(); int per_image_with = 10; // 每张碎片图片的宽度 int per_image_height = 58; // 每张碎片图片的高度 List&lt;BufferedImage&gt; upperList = new ArrayList&lt;&gt;(); List&lt;BufferedImage&gt; downList = new ArrayList&lt;&gt;(); // 将原始图片裁剪成碎片 for (Map&lt;String, Integer&gt; location : locations) &#123; int x = location.get("x"); int y = location.get("y"); if (y == -58) &#123; upperList.add(image.getSubimage(abs(x), 58, per_image_with, per_image_height)); &#125; else if (y == 0) &#123; downList.add(image.getSubimage(abs(x), 0, per_image_with, per_image_height)); &#125; &#125; BufferedImage newImage = new BufferedImage(upperList.size() * per_image_with, image.getHeight(), image.getType()); // 重绘图片的上半部分 int x_offset = 0; for (BufferedImage bufferedImage : upperList) &#123; Graphics graphics = newImage.getGraphics(); graphics.drawImage(bufferedImage, x_offset, 0, null); x_offset += bufferedImage.getWidth(); &#125; // 重绘图片的下半部分 x_offset = 0; for (BufferedImage bufferedImage : downList) &#123; Graphics graphics = newImage.getGraphics(); graphics.drawImage(bufferedImage, x_offset, 58, null); x_offset += bufferedImage.getWidth(); &#125; log.debug("还原图片耗时：&#123;&#125;ms", System.currentTimeMillis() - begin); return newImage;&#125; 找到背景图中缺口的位置 正常图片： 缺口图片： Geetest验证时，只需要将拼图块水平移动到正确的位置即可。因此通过上面两张图片比 对，发现我们只要找到缺口的x坐标即可。 这里我借用的参考内容的方法：两张原始图的大小都是相同的 260*116，那就通过两个 for 循环依次对比每个像素点的 RGB 值，如果相差超过 50 则就认为找到了缺口的位置。 12345678910111213141516171819202122232425262728293031323334353637/** * 计算验证图的缺口位置（x轴） 两张原始图的大小都是相同的260*116，那就通过两个for循环依次对比每个像素点的RGB值， 如果RGB三元素中有一个相差超过50则就认为找到了缺口的位置 * * @param image1 图像1 * @param image2 图像2 * @return 缺口的x坐标 */public static int getDiffX(BufferedImage image1, BufferedImage image2) &#123; long begin = System.currentTimeMillis(); for (int x = 0; x &lt; image1.getWidth(); x++) &#123; for (int y = 0; y &lt; image1.getHeight(); y++) &#123; if (!isSimilar(image1, image2, x, y)) &#123; return x; &#125; &#125; &#125; log.debug("图片对比耗时：&#123;&#125;ms", System.currentTimeMillis() - begin); return 0;&#125;/** * 判断image1, image2的[x, y]这一像素是否相似，如果该像素的RGB值相差都在50以内，则认为相似。 * * @param image1 图像1 * @param image2 图像2 * @param x_offset x坐标 * @param y_offset y坐标 * @return 是否相似 */public static boolean isSimilar(BufferedImage image1, BufferedImage image2, int x_offset, int y_offset) &#123; Color pixel1 = new Color(image1.getRGB(x_offset, y_offset)); Color pixel2 = new Color(image2.getRGB(x_offset, y_offset)); return abs(pixel1.getBlue() - pixel2.getBlue()) &lt; 50 &amp;&amp; abs(pixel1.getGreen() - pixel2.getGreen()) &lt; 50 &amp;&amp; abs(pixel1.getRed() - pixel2.getRed()) &lt; 50;&#125; 将滑块拖拽到缺口Geetest在拖拽拼图块时，对拼图块的移动轨迹做了一些 “ 人工智能 ” 的算法识别，使 用程序直接将拼图块拖入缺口时，发现有很大的概率会被识别为机器操作，页面显示怪物 吃掉了饼图。因此我们需要采用一些算法来模拟人拖拽的行为。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166/** * 根据缺口位置x_offset，仿照手动拖动滑块时的移动轨迹。 * 手动拖动滑块有几个特点： * 开始时拖动速度快，最后接近目标时会慢下来； * 总时间大概1~3秒； * 有可能会拖超过后再拖回头； * * @return 返回一个轨迹数组，数组中的每个轨迹都是[x,y,z]三元素：x代表横向位移，y代表竖向位移，z代表时间间隔，单位毫秒 */private static List&lt;Map&lt;String, Integer&gt;&gt; getTrack(int x_offset) &#123; List&lt;Map&lt;String, Integer&gt;&gt; tracks; long begin = System.currentTimeMillis(); // 实际上滑块的起始位置并不是在图像的最左边，而是大概有6个像素的距离，所以滑动距离要减掉这个长度 x_offset = x_offset - 6; if (getRandom(0, 10) % 2 == 0) &#123; tracks = strategics_1(x_offset); &#125; else &#123; tracks = strategics_2(x_offset); &#125; log.debug("生成轨迹耗时: &#123;&#125;ms", System.currentTimeMillis() - begin); log.debug("计算出移动轨迹: &#123;&#125;", tracks); return tracks;&#125;/** * 轨迹策略1 */private static List&lt;Map&lt;String, Integer&gt;&gt; strategics_1(int x_offset) &#123; List&lt;Map&lt;String, Integer&gt;&gt; tracks = new ArrayList&lt;&gt;(); float totalTime = 0; int x = getRandom(1, 3); // 随机按1~3的步长生成各个点 while (x_offset - x &gt;= 5) &#123; Map&lt;String, Integer&gt; point = new HashMap&lt;&gt;(3); point.put("x", x); point.put("y", 0); point.put("z", 0); tracks.add(point); x_offset = x_offset - x; x = getRandom(1, 5); totalTime += point.get("z").floatValue(); &#125; // 后面几个点放慢时间 for (int i = 0; i &lt; x_offset; i++) &#123; Map&lt;String, Integer&gt; point = new HashMap&lt;&gt;(3); point.put("x", 1); point.put("y", 0); point.put("z", getRandom(10, 200)); tracks.add(point); totalTime += point.get("z").floatValue(); &#125; log.debug("预计拖拽耗时: &#123;&#125;ms", totalTime); return tracks;&#125;/** * 轨迹策略2 */private static List&lt;Map&lt;String, Integer&gt;&gt; strategics_2(int x_offset) &#123; List&lt;Map&lt;String, Integer&gt;&gt; tracks = new ArrayList&lt;&gt;(); float totalTime = 0; int dragX = 0; // 已拖拽的横向偏移量 int nearRange = getRandom(5, 10); // 靠近缺口的范围 while (dragX &lt; x_offset - nearRange) &#123; // 生成快速拖拽点，拖拽距离非常靠近切口 int stepLength = getRandom(1, 5); // 随机按1~5的步长生成各个点 Map&lt;String, Integer&gt; point = new HashMap&lt;&gt;(3); point.put("x", stepLength); point.put("y", 0); point.put("z", getRandom(0, 2)); tracks.add(point); totalTime += point.get("z").floatValue(); dragX += stepLength; &#125; // 随机一定的比例将滑块拖拽过头 if (getRandom(0, 99) % 2 == 0) &#123; int stepLength = getRandom(10, 15); // 随机按1~5的步长生成各个点 Map&lt;String, Integer&gt; attachPoint = new HashMap&lt;&gt;(3); attachPoint.put("x", stepLength); attachPoint.put("y", 0); attachPoint.put("z", getRandom(0, 2)); tracks.add(attachPoint); dragX += stepLength; totalTime += attachPoint.get("z").floatValue(); &#125; // 精确点 for (int i = 0; i &lt; Math.abs(dragX - x_offset); i++) &#123; if (dragX &gt; x_offset) &#123; Map&lt;String, Integer&gt; point = new HashMap&lt;&gt;(3); point.put("x", -1); point.put("y", 0); point.put("z", getRandom(10, 100)); tracks.add(point); totalTime += point.get("z").floatValue(); &#125; else &#123; Map&lt;String, Integer&gt; point = new HashMap&lt;&gt;(3); point.put("x", 1); point.put("y", 0); point.put("z", getRandom(10, 100)); tracks.add(point); totalTime += point.get("z").floatValue(); &#125; &#125; log.debug("预计拖拽耗时: &#123;&#125;ms", totalTime); return tracks;&#125;/** * 根据移动轨迹，模拟拖动极验的验证滑块 */private static boolean simulateDrag(WebDriver webDriver, Site site, List&lt;Map&lt;String, Integer&gt;&gt; tracks) throws InterruptedException &#123; log.debug("开始模拟拖动滑块"); WebElement slider = webDriver.findElement(By.cssSelector(site.getGeetest().getSliderKnob())); log.debug("滑块初始位置: &#123;&#125;", slider.getLocation()); Actions actions = new Actions(webDriver); actions.clickAndHold(slider).perform(); for (Map&lt;String, Integer&gt; point : tracks) &#123; int x = point.get("x") + 22; int y = point.get("y") + 22; actions.moveToElement(slider, x, y).perform(); int z = point.get("z"); TimeUnit.MILLISECONDS.sleep(z); &#125; TimeUnit.MILLISECONDS.sleep(getRandom(100, 200)); // 随机停顿100~200毫秒 actions.release(slider).perform(); TimeUnit.MILLISECONDS.sleep(100); // 等待0.1秒后检查结果 try &#123; // 在5秒之内检查弹出框是否消失，如果消失则说明校验通过；如果没有消失说明校验失败。 new WebDriverWait(webDriver, 5).until((ExpectedCondition&lt;Boolean&gt;) driver -&gt; &#123; try &#123; WebElement popupElement = driver.findElement(By.cssSelector(site.getGeetest().getPopupWrap())); return !popupElement.isDisplayed(); &#125; catch (NoSuchElementException e) &#123; return true; // 元素不存在也返回true &#125; &#125;); return true; &#125; catch (Exception e) &#123; return false; &#125;&#125; 总结解决好上面的三点之后，Geetest验证基本就破解掉了，但是要爬取企业信用信息公示系统中的数据还有一些事情要做。目前工商局有多个地区系统，每个系统的页面解析处理也不一样。某些地区系统甚至还针对访问频次做了控制，你可以通过 IP 代理来解决。 完整的代码示例请参考: https://github.com/aqlu/geetest-crack 运行效果： 本文的参考资料：http://blog.csdn.net/paololiu/article/details/52514504]]></content>
      <categories>
        <category>技术笔记</category>
        <category>工具类</category>
      </categories>
      <tags>
        <tag>geetest</tag>
        <tag>验证码破解</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MacOS科学上网]]></title>
    <url>%2F%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0%2F%E5%B7%A5%E5%85%B7%E7%B1%BB%2FMacOs-scientific-Surf-the-Internet.html</url>
    <content type="text"><![CDATA[本文将介绍如何在MacOS上采用VMess协议与墙外VPS通信，其中主要用到了v2ray这款开源工具。服务端安装(Centos)因为我选择的VPS的Centos的操作系统，所以此章节主要基于Centos操作系统来描述。安装包下载：wget https://github.com/v2ray/v2ray-core/releases/download/v2.40/v2ray-linux-64.zip解压安装包到/opt目录：unzip v2ray-linux-64.zip -d /opt编写配置文件:12cd /opt/v2ray-v2.40-linux-64vi config.json config.json内容： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950&#123; "log" : &#123; // 日志输出配置 "access": "/var/log/v2ray/access.log", "error": "/var/log/v2ray/error.log", "loglevel": "warning" &#125;, // 对外提供 vmess 协议 "inbound": &#123; "port": 28399, // 监听端口 "protocol": "vmess", "settings": &#123; "clients": [&#123; "id": "3b129ddd-72a3-4d28-aeee-028a0fe86e33", // 可以随机生成一个，但必须保证服务端ID与客户端ID必须保持一致 "level": 1, "alterId": 64, "security": "aes-128-gcm" &#125;] &#125; &#125;, "outbound": &#123; "protocol": "freedom", "settings": &#123;&#125; &#125;, "routing": &#123; "strategy": "rules", "settings": &#123; "rules": [&#123; "type": "field", "ip": [ "0.0.0.0/8", "10.0.0.0/8", "100.64.0.0/10", "127.0.0.0/8", "169.254.0.0/16", "172.16.0.0/12", "192.0.0.0/24", "192.0.2.0/24", "192.168.0.0/16", "198.18.0.0/15", "198.51.100.0/24", "203.0.113.0/24", "::1/128", "fc00::/7", "fe80::/10" ], "outboundTag": "blocked" &#125;] &#125; &#125;&#125; 启动v2ray服务：/opt/v2ray-v2.40-linux-64/v2ray -config=/opt/v2ray-v2.40-linux-64/config.json &gt; v2ray.log &amp; 客户端安装(MacOs) 安装包下载：wget https://github.com/v2ray/v2ray-core/releases/download/v2.40/v2ray-macos.zip 解压安装包到/usr/local/opt目录：unzip v2ray-linux-64.zip -d /usr/local/opt 编写配置文件: 12cd /usr/local/opt/v2ray-v2.40-linux-64vi config.json config.json内容： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354&#123; "log" : &#123; "access": "/var/log/v2ray/access.log", "error": "/var/log/v2ray/error.log", "loglevel": "warning" &#125;, // 对外提供 socks5 协议 "inbound": &#123; "port": 1080, // 监听端口 "protocol": "socks", // 入口协议为 SOCKS 5 "settings": &#123; "auth": "noauth" //socks的认证设置，noauth 代表不认证，由于 socks 通常在客户端使用，所以这里不认证 &#125; &#125;, "outbound": &#123; "protocol": "vmess", // 出口协议，对应服务端的入口协议 "settings": &#123; "vnext": [&#123; "address": "serveraddr.com", // 服务器地址，请修改为你自己的服务器 ip 或域名 "port": 28399, // 服务器端口，对应服务器端入口的端口 "users": [&#123; "id": "3b129ddd-72a3-4d28-aeee-028a0fe86e33", // 用户ID, 服务端ID与客户端ID必须保持一致 "alterId": 64 // 此处的值也应当与服务器相同 &#125;] &#125;] &#125; &#125;, "routing": &#123; "strategy": "rules", "settings": &#123; "rules": [&#123; "type": "field", "ip": [ "0.0.0.0/8", "10.0.0.0/8", "100.64.0.0/10", "127.0.0.0/8", "169.254.0.0/16", "172.16.0.0/12", "192.0.0.0/24", "192.0.2.0/24", "192.168.0.0/16", "198.18.0.0/15", "198.51.100.0/24", "203.0.113.0/24", "::1/128", "fc00::/7", "fe80::/10" ], "outboundTag": "blocked" &#125;] &#125; &#125;&#125; 启动v2ray服务：/usr/local/opt/v2ray-v2.40-linux-64/v2ray -config=/usr/local/opt/v2ray-v2.40-linux-64/config.json &gt; v2ray.log &amp; 客户端安装的另一种方式（基于Docker安装） 下载镜像： docker pull v2ray/official 编写配置文件： 123mkdir -p ~/v2raycd ~/v2rayvi config.json # 内容请参考客户端安装(MacOs) 创建容器： 1docker run -d --restart=always --name v2ray -v ~/v2ray:/etc/v2ray -p 1080:1080 v2ray/official v2ray -config=/etc/v2ray/config.json -p 后面跟的端口为config.json中inbound配置的监听端口 校验容器是否创建成功：docker container ls 启动容器：docker start 停止v2ray：docker stop v2ray 重启v2ray：docker restart v2ray 查看日志：docker container log v2ray 更多参考资料 项目：https://github.com/v2ray/v2ray-core 用户手册：https://www.v2ray.com/ 其它文档：https://toutyrater.github.io/ 可能遇到的问题 客户端版本与服务端版本不匹配是，可能会出现服务端报错：rejected Proxy|VMess|Encoding: invalid auth。]]></content>
      <categories>
        <category>技术笔记</category>
        <category>工具类</category>
      </categories>
      <tags>
        <tag>vmess</tag>
        <tag>翻墙</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World!]]></title>
    <url>%2F%2Fhello-world.html</url>
    <content type="text"><![CDATA[Hello World!这是我的第一篇博客，感谢Hexo提供了一个简单好用的博客框架，感谢NexT优雅的的主题。此博客是本人的一个杂记。为什么叫杂记？其实就是一些乱七八糟的东西，可能是曾经的一些技术笔记，也可能是一些技术专题的连载，也可能是一些生活观点与琐事。]]></content>
  </entry>
</search>
